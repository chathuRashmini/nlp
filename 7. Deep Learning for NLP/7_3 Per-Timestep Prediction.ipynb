{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as random\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Softmax, Dropout, SimpleRNN, Embedding, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "UNK_TOKEN = '__unk__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "In this exercise, we will be doing Part-of-Speech tag prediction for a sequence of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 209772 samples\n"
     ]
    }
   ],
   "source": [
    "POS_TAGS = {\n",
    "\t'NOTAG': 0,\n",
    "\t'#': 1,\n",
    "\t'$': 2,\n",
    "\t'&apos;&apos;': 3,\n",
    "\t',': 4,\n",
    "\t'-RRB-': 5,\n",
    "\t'.': 6,\n",
    "\t':': 7,\n",
    "\t'CC': 8,\n",
    "\t'CD': 9,\n",
    "\t'DT': 10,\n",
    "\t'EX': 11,\n",
    "\t'FW': 12,\n",
    "\t'IN': 13,\n",
    "\t'JJ': 14,\n",
    "\t'JJR': 15,\n",
    "\t'JJS': 16,\n",
    "\t'LS': 17,\n",
    "\t'MD': 18,\n",
    "\t'NN': 19,\n",
    "\t'NNP': 20,\n",
    "\t'NNPS': 21,\n",
    "\t'NNS': 22,\n",
    "\t'PDT': 23,\n",
    "\t'POS': 24,\n",
    "\t'PRP': 25,\n",
    "\t'PRP$': 26,\n",
    "\t'RB': 27,\n",
    "\t'RBR': 28,\n",
    "\t'RBS': 29,\n",
    "\t'RP': 30,\n",
    "\t'TO': 31,\n",
    "\t'UH': 32,\n",
    "\t'VB': 33,\n",
    "\t'VBD': 34,\n",
    "\t'VBG': 35,\n",
    "\t'VBN': 36,\n",
    "\t'VBP': 37,\n",
    "\t'VBZ': 38,\n",
    "\t'WDT': 39,\n",
    "\t'WP': 40,\n",
    "\t'WP$': 41,\n",
    "\t'WRB': 42,\n",
    "\t'``': 43\n",
    "} \n",
    "\n",
    "text = []\n",
    "labels = []\n",
    "with open('data/text.en.txt', encoding='utf-8') as fp:\n",
    "    for line in fp:\n",
    "        text.append(line.strip().split(' '))\n",
    "\n",
    "with open('data/labels.en.txt') as fp:\n",
    "    for line in fp:\n",
    "        labels.append([POS_TAGS[p] for p in line.strip().split(' ')])\n",
    "        \n",
    "assert(len(text) == len(labels))\n",
    "for d, l in zip(text, labels):\n",
    "    assert(len(d) == len(l))\n",
    "    \n",
    "data = [(d, l) for d,l in zip(text, labels)]\n",
    "\n",
    "print(\"Loaded %d samples\"%(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['&lt;', 'description', '&gt;', 'TED', 'Talk', 'Subtitles', 'and', 'Transcript', ':', 'With', 'vibrant', 'video', 'clips', 'captured', 'by', 'submarines', ',', 'David', 'Gallo', 'takes', 'us', 'to', 'some', 'of', 'Earth', '&apos;s', 'darkest', ',', 'most', 'violent', ',', 'toxic', 'and', 'beautiful', 'habitats', ',', 'the', 'valleys', 'and', 'volcanic', 'ridges', 'of', 'the', 'oceans', '&apos;', 'depths', ',', 'where', 'life', 'is', 'bizarre', ',', 'resilient', 'and', 'shockingly', 'abundant', '.', '&lt;', '/', 'description', '&gt;'], [10, 19, 13, 36, 19, 22, 8, 19, 7, 13, 14, 19, 22, 36, 13, 22, 4, 20, 20, 38, 25, 31, 10, 13, 20, 24, 16, 4, 29, 14, 4, 14, 8, 14, 22, 4, 10, 22, 8, 14, 22, 13, 10, 22, 24, 22, 4, 42, 19, 38, 14, 4, 14, 8, 27, 14, 6, 3, 20, 19, 6])\n"
     ]
    }
   ],
   "source": [
    "print(data[5]) # contains the tokens followed by their corresponding POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Instances: 146840\n",
      "Dev Instances: 20977\n",
      "Test Instances: 41954\n"
     ]
    }
   ],
   "source": [
    "# Send random seed for reproducible results\n",
    "random.seed(5)\n",
    "random.shuffle(data)\n",
    "\n",
    "total_instances = len(data)\n",
    "num_train_instances = int(0.7 * total_instances)\n",
    "num_dev_instances = int(0.1 * total_instances)\n",
    "num_test_instances = int(0.2 * total_instances)\n",
    "\n",
    "train = data[:num_train_instances]\n",
    "dev = data[num_train_instances:num_train_instances + num_dev_instances]\n",
    "test = data[num_train_instances + num_dev_instances:num_train_instances + num_dev_instances + num_test_instances]\n",
    "\n",
    "print(\"Train Instances: %d\"%(len(train)))\n",
    "print(\"Dev Instances: %d\"%(len(dev)))\n",
    "print(\"Test Instances: %d\"%(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [d for d,_ in train]\n",
    "train_labels = [l for _,l in train]\n",
    "\n",
    "dev_data = [d for d,_ in dev]\n",
    "dev_labels = [l for _,l in dev]\n",
    "\n",
    "test_data = [d for d,_ in test]\n",
    "test_labels = [l for _,l in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 54562\n",
      "Most frequent tokens\n",
      "\t,: 173469\n",
      "\t.: 138735\n",
      "\tthe: 109915\n",
      "\tto: 68599\n",
      "\tof: 64398\n",
      "\tand: 59512\n",
      "\ta: 57597\n",
      "\tthat: 48974\n",
      "\tI: 44784\n",
      "\tin: 40624\n",
      "Least frequent tokens\n",
      "\tincapacitates: 1\n",
      "\tbankruptcies: 1\n",
      "\tIPOs: 1\n",
      "\tes: 1\n",
      "\tDar: 1\n",
      "\tSeparate: 1\n",
      "\tsquashed: 1\n",
      "\traking: 1\n",
      "\tHeroin: 1\n",
      "\tAnticipation: 1\n"
     ]
    }
   ],
   "source": [
    "# Prepare vocabulary\n",
    "full_vocab = dict()\n",
    "for instance in train_data:\n",
    "    for token in instance:\n",
    "        full_vocab[token] = 1 + full_vocab.get(token, 0)\n",
    "\n",
    "# Sort vocabulary by occurence\n",
    "sorted_vocab = sorted(full_vocab.keys(), key=lambda word: -full_vocab[word])\n",
    "\n",
    "# Print some samples\n",
    "print(\"Vocabulary size: %d\"%(len(sorted_vocab)))\n",
    "print(\"Most frequent tokens\")\n",
    "for i in range(10):\n",
    "    print(\"\\t%s: %d\"%(sorted_vocab[i], full_vocab[sorted_vocab[i]]))\n",
    "print(\"Least frequent tokens\")\n",
    "for i in range(1,11):\n",
    "    print(\"\\t%s: %d\"%(sorted_vocab[-i], full_vocab[sorted_vocab[-i]]))\n",
    "\n",
    "# We can choose to limit the vocab_size here to only a portion of the original vocab,\n",
    "# i.e. ignore infrequent tokens to save on memory\n",
    "vocab_size = VOCAB_SIZE\n",
    "    \n",
    "# Create final vocab\n",
    "word2idx = {w: idx for idx, w in enumerate(sorted_vocab[:vocab_size])}\n",
    "idx2word = {idx: w for idx, w in enumerate(sorted_vocab[:vocab_size])}\n",
    "\n",
    "\n",
    "word2idx[UNK_TOKEN] = vocab_size\n",
    "idx2word[vocab_size] = UNK_TOKEN\n",
    "vocab_size = vocab_size + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter text based on vocabulary\n",
    "We will now have to replace words we do not have in the vocabulary with a special token, `__unk__` in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens filtered out as unknown:\n",
      "Train: 120360/2988546\n",
      "Dev: 18232/426127\n",
      "Test: 36205/854125\n"
     ]
    }
   ],
   "source": [
    "train_data = [[t if t in word2idx else UNK_TOKEN for t in instance] for instance in train_data]\n",
    "dev_data = [[t if t in word2idx else UNK_TOKEN for t in instance] for instance in dev_data]\n",
    "test_data = [[t if t in word2idx else UNK_TOKEN for t in instance] for instance in test_data]\n",
    "\n",
    "print(\"Number of tokens filtered out as unknown:\")\n",
    "print(\"Train: %d/%d\"%(len([1 for instance in train_data for t in instance if t == UNK_TOKEN]), sum([len(i) for i in train_data])))\n",
    "print(\"Dev: %d/%d\"%(len([1 for instance in dev_data for t in instance if t == UNK_TOKEN]), sum([len(i) for i in dev_data])))\n",
    "print(\"Test: %d/%d\"%(len([1 for instance in test_data for t in instance if t == UNK_TOKEN]), sum([len(i) for i in test_data])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data in tensor form\n",
    "Our keras models finally take tensors as input and labels, so we need to modify our data to fit this form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data_to_tensor\n",
    "# Given a list of instances, where each instance is a list of tokens,\n",
    "# this function does the following:\n",
    "# 1: Replace each token with its corresponding index\n",
    "# 2: Pad sequences to MAX_SEQUENCE_LENGTH (or truncate them if longer)\n",
    "#       Padding is done with a unique element, in this case `vocab_size`\n",
    "#       The network will learn that this unique element is padding and does not\n",
    "#        mean anything semantically\n",
    "# 3: Package everything nicely as a NUM_INSTANCES x MAX_SEQUENCE_LENGTH matrix\n",
    "def data_to_tensor(data, pad_value=vocab_size):\n",
    "    # First convert from words to indices\n",
    "    idx_data = [[word2idx[t] for t in instance] for instance in data]\n",
    "    \n",
    "    # Create numpy representation\n",
    "    return pad_sequences([np.array(d) for d in idx_data], maxlen=MAX_SEQUENCE_LENGTH, value=pad_value)\n",
    "\n",
    "X_train = data_to_tensor(train_data)\n",
    "y_train = to_categorical(pad_sequences(train_labels, maxlen=MAX_SEQUENCE_LENGTH, value=POS_TAGS['NOTAG']))\n",
    "\n",
    "X_dev = data_to_tensor(dev_data)\n",
    "y_dev = to_categorical(pad_sequences(dev_labels, maxlen=MAX_SEQUENCE_LENGTH, value=POS_TAGS['NOTAG']))\n",
    "\n",
    "X_test = data_to_tensor(test_data)\n",
    "y_test = to_categorical(pad_sequences(test_labels, maxlen=MAX_SEQUENCE_LENGTH, value=POS_TAGS['NOTAG']))\n",
    "\n",
    "vocab_size = vocab_size + 1 # Add 1 for the padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146840, 100)\n",
      "(146840, 100, 44)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 25)           250050    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 100, 30)           1680      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 100, 44)           1364      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 100, 44)           0         \n",
      "=================================================================\n",
      "Total params: 253,094\n",
      "Trainable params: 253,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(output_dim=25, input_dim=vocab_size, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(SimpleRNN(30, return_sequences=True)) # Return output at every timestep\n",
    "# Output of Simple RNN is of size 100x30\n",
    "# We can't use a dense layer after this since that would take only a single output (summary or average)\n",
    "model.add(TimeDistributed(Dense(len(POS_TAGS)))) # Apply dense layer for each timestep\n",
    "# Output of TimeDistributed layer is 100x40 (40 being the # of POS tags)\n",
    "model.add(TimeDistributed(Softmax()))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "41954/41954 [==============================] - 27s 643us/step\n",
      "Test Set Accuracy: 0.51%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Set Accuracy: %0.2f%%\"%(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
