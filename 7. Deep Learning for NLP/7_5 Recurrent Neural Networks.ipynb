{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Softmax, Dropout\n",
    "from keras.layers import SimpleRNN, LSTM, Embedding, Bidirectional, GlobalAveragePooling1D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 200 # We expect all sentences to be less than 200 tokens long\n",
    "VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "In this exercise, we will use a smaller dataset that has been preprocessing already by the Keras folks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\",\n",
    "                                                         num_words=VOCAB_SIZE,\n",
    "                                                         skip_top=0,\n",
    "                                                         maxlen=MAX_SEQUENCE_LENGTH,\n",
    "                                                         test_split=0.5,\n",
    "                                                         seed=113,\n",
    "                                                         start_char=1,\n",
    "                                                         oov_char=2,\n",
    "                                                         index_from=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   15   17   12]\n",
      " [   0    0    0 ...  505   17   12]\n",
      " [   0    0    0 ...   11   17   12]\n",
      " ...\n",
      " [   0    0    0 ...  254   17   12]\n",
      " [   0    0    0 ... 2735   17   12]\n",
      " [   0    0    0 ... 4329   17   12]]\n"
     ]
    }
   ],
   "source": [
    "# News articles padded with zeros (in front here) to make 200 input vector (max sentence length)\n",
    "# The 200 corresponds to the number of time steps in the RNN\n",
    "# Default in Keras is to pad in front!\n",
    "X_train = pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH, value=0) \n",
    "X_test = pad_sequences(x_test, maxlen=MAX_SEQUENCE_LENGTH, value=0)\n",
    "\n",
    "print(X_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/dl4nlp/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/envs/dl4nlp/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 4201 samples, validate on 222 samples\n",
      "Epoch 1/10\n",
      "4201/4201 [==============================] - 8s 2ms/step - loss: 2.9413 - acc: 0.3483 - val_loss: 1.9921 - val_acc: 0.4820\n",
      "Epoch 2/10\n",
      "4201/4201 [==============================] - 7s 2ms/step - loss: 2.2202 - acc: 0.4085 - val_loss: 1.8861 - val_acc: 0.4820\n",
      "Epoch 3/10\n",
      "4201/4201 [==============================] - 7s 2ms/step - loss: 2.1404 - acc: 0.4090 - val_loss: 1.8415 - val_acc: 0.4820\n",
      "Epoch 4/10\n",
      "4201/4201 [==============================] - 7s 2ms/step - loss: 2.0591 - acc: 0.4259 - val_loss: 1.7972 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "4201/4201 [==============================] - 7s 2ms/step - loss: 1.8445 - acc: 0.4725 - val_loss: 1.7147 - val_acc: 0.5541\n",
      "Epoch 6/10\n",
      "4201/4201 [==============================] - 7s 2ms/step - loss: 1.6553 - acc: 0.5451 - val_loss: 1.6730 - val_acc: 0.5811\n",
      "Epoch 7/10\n",
      "4201/4201 [==============================] - 7s 2ms/step - loss: 1.4913 - acc: 0.6063 - val_loss: 2.0869 - val_acc: 0.4550\n",
      "Epoch 8/10\n",
      "4201/4201 [==============================] - 7s 2ms/step - loss: 1.3639 - acc: 0.6463 - val_loss: 1.7630 - val_acc: 0.5225\n",
      "Epoch 9/10\n",
      "4201/4201 [==============================] - 7s 2ms/step - loss: 1.2201 - acc: 0.6817 - val_loss: 2.1039 - val_acc: 0.4550\n",
      "Epoch 10/10\n",
      "4201/4201 [==============================] - 7s 2ms/step - loss: 1.1161 - acc: 0.7108 - val_loss: 1.9281 - val_acc: 0.5135\n",
      "4423/4423 [==============================] - 2s 518us/step\n",
      "Test accuracy: 43.45%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Special dense layer that does word embeddings - auto creates idx mapping\n",
    "# Length of our embeddings here is 10 - we feel 10 dimensions is sufficient to capture model\n",
    "model.add(Embedding(VOCAB_SIZE, 10, input_length=MAX_SEQUENCE_LENGTH)) \n",
    "# We specify that the RNN should have 25 hidden neurons; returns a vector of 25 at the end (summary)\n",
    "model.add(SimpleRNN(25)) \n",
    "model.add(Dense(46)) # Inputs 25 and outputs 46 (the number of classes we have)\n",
    "model.add(Softmax())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "model.fit(X_train, to_categorical(y_train), epochs=10, validation_split=0.05)\n",
    "loss, acc = model.evaluate(X_test, to_categorical(y_test))\n",
    "print(\"Test accuracy: %0.2f%%\"%(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4201 samples, validate on 222 samples\n",
      "Epoch 1/5\n",
      "4201/4201 [==============================] - 8s 2ms/step - loss: 2.9905 - acc: 0.3732 - val_loss: 2.0642 - val_acc: 0.4820\n",
      "Epoch 2/5\n",
      "4201/4201 [==============================] - 7s 2ms/step - loss: 2.2538 - acc: 0.4085 - val_loss: 1.9187 - val_acc: 0.4820\n",
      "Epoch 3/5\n",
      "4201/4201 [==============================] - 7s 2ms/step - loss: 2.1475 - acc: 0.4151 - val_loss: 1.8563 - val_acc: 0.4820\n",
      "Epoch 4/5\n",
      "4201/4201 [==============================] - 8s 2ms/step - loss: 2.0613 - acc: 0.4254 - val_loss: 1.8644 - val_acc: 0.4865\n",
      "Epoch 5/5\n",
      "4201/4201 [==============================] - 8s 2ms/step - loss: 1.9103 - acc: 0.4575 - val_loss: 1.7475 - val_acc: 0.5045\n",
      "4423/4423 [==============================] - 3s 595us/step\n",
      "Test accuracy: 43.43%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, 10, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Bidirectional(SimpleRNN(25), merge_mode='ave'))\n",
    "model.add(Dense(46))\n",
    "model.add(Softmax())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(X_train, to_categorical(y_train), epochs=5, validation_split=0.05)\n",
    "loss, acc = model.evaluate(X_test, to_categorical(y_test))\n",
    "print(\"Test accuracy: %0.2f%%\"%(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN with averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4201 samples, validate on 222 samples\n",
      "Epoch 1/5\n",
      "4201/4201 [==============================] - 8s 2ms/step - loss: 2.6913 - acc: 0.3682 - val_loss: 1.9274 - val_acc: 0.4820\n",
      "Epoch 2/5\n",
      "4201/4201 [==============================] - 8s 2ms/step - loss: 2.2078 - acc: 0.4090 - val_loss: 1.8891 - val_acc: 0.4910\n",
      "Epoch 3/5\n",
      "4201/4201 [==============================] - 8s 2ms/step - loss: 2.0514 - acc: 0.4378 - val_loss: 1.6636 - val_acc: 0.5270\n",
      "Epoch 4/5\n",
      "4201/4201 [==============================] - 8s 2ms/step - loss: 1.8975 - acc: 0.4875 - val_loss: 1.5868 - val_acc: 0.6036\n",
      "Epoch 5/5\n",
      "4201/4201 [==============================] - 8s 2ms/step - loss: 1.8149 - acc: 0.5273 - val_loss: 1.5251 - val_acc: 0.5946\n",
      "4423/4423 [==============================] - 2s 561us/step\n",
      "Test accuracy: 51.80%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, 10, input_length=MAX_SEQUENCE_LENGTH))\n",
    "# Instead of returning the summary vector - we ask RNN to return vectors at each RNN unit\n",
    "model.add(SimpleRNN(25, return_sequences=True))\n",
    "# We ask the 25 output vectors to be averaged\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(46))\n",
    "model.add(Softmax())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "model.fit(X_train, to_categorical(y_train), epochs=5, validation_split=0.05)\n",
    "loss, acc = model.evaluate(X_test, to_categorical(y_test))\n",
    "print(\"Test accuracy: %0.2f%%\"%(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4201 samples, validate on 222 samples\n",
      "Epoch 1/5\n",
      "4201/4201 [==============================] - 56s 13ms/step - loss: 2.6511 - acc: 0.4054 - val_loss: 1.9272 - val_acc: 0.4820\n",
      "Epoch 2/5\n",
      "4201/4201 [==============================] - 53s 12ms/step - loss: 2.2251 - acc: 0.4085 - val_loss: 1.9224 - val_acc: 0.4820\n",
      "Epoch 3/5\n",
      "4201/4201 [==============================] - 53s 13ms/step - loss: 2.2217 - acc: 0.4085 - val_loss: 1.9165 - val_acc: 0.4820\n",
      "Epoch 4/5\n",
      "2560/4201 [=================>............] - ETA: 19s - loss: 2.2208 - acc: 0.4117"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, 10, input_length=MAX_SEQUENCE_LENGTH))\n",
    "# We ask keras to 'merge' (average) the vectors of both directions to send it to the next layer\n",
    "model.add(Bidirectional(LSTM(25, return_sequences=True), merge_mode='ave'))\n",
    "# The final bidirectional layer only needs the summary vector\n",
    "model.add(Bidirectional(LSTM(25), merge_mode='ave'))\n",
    "model.add(Dense(46))\n",
    "model.add(Softmax())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "model.fit(X_train, to_categorical(y_train), epochs=5, validation_split=0.05)\n",
    "loss, acc = model.evaluate(X_test, to_categorical(y_test))\n",
    "print(\"Test accuracy: %0.2f%%\"%(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
