{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7108344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document for extracting NEs from\n",
    "text = \"\"\"\n",
    "Bayern Munich, or FC Bayern, is a German sports club based in Munich, \n",
    "Bavaria, Germany. It is best known for its professional football team, \n",
    "which plays in the Bundesliga, the top tier of the German football \n",
    "league system, and is the most successful club in German football \n",
    "history, having won a record 26 national titles and 18 national cups. \n",
    "FC Bayern was founded in 1900 by eleven football players led by Franz John. \n",
    "Although Bayern won its first national championship in 1932, the club \n",
    "was not selected for the Bundesliga at its inception in 1963. The club \n",
    "had its period of greatest success in the middle of the 1970s when, \n",
    "under the captaincy of Franz Beckenbauer, it won the European Cup three \n",
    "times in a row (1974-76). Overall, Bayern has reached ten UEFA Champions \n",
    "League finals, most recently winning their fifth title in 2013 as part \n",
    "of a continental treble. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae0d2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from normalization import parse_document\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca2ecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences using our helper functions in normalization.py\n",
    "sentences = parse_document(text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6964f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tag the sentences and use nltk's Named Entity Chunker\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "ne_chunked_sents = [nltk.ne_chunk(tagged) for tagged in tagged_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9558fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all named entities\n",
    "named_entities = []\n",
    "for ne_tagged_sentence in ne_chunked_sents:\n",
    "    for tagged_tree in ne_tagged_sentence:\n",
    "        if hasattr(tagged_tree, 'label'):\n",
    "                entity_name = ' '.join(c[0] for c in tagged_tree.leaves())\n",
    "                entity_type = tagged_tree.label()\n",
    "                named_entities.append((entity_name, entity_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53ceecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique named entities - why do this?\n",
    "named_entities = list(set(named_entities))\n",
    "# Store named entities in a (pandas) data frame (for pretty printing)\n",
    "entity_frame = pd.DataFrame(named_entities, \n",
    "                            columns=['Entity Name', 'Entity Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d08b785d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity extraction using NLTK's NE Recognizer:\n",
      "          Entity Name   Entity Type\n",
      "0   Franz Beckenbauer        PERSON\n",
      "1             Bavaria           GPE\n",
      "2           FC Bayern  ORGANIZATION\n",
      "3            European  ORGANIZATION\n",
      "4             Germany           GPE\n",
      "5             Overall           GPE\n",
      "6              Bayern        PERSON\n",
      "7              German           GPE\n",
      "8              Bayern           GPE\n",
      "9          Franz John        PERSON\n",
      "10         Bundesliga  ORGANIZATION\n",
      "11               UEFA  ORGANIZATION\n",
      "12             Munich  ORGANIZATION\n",
      "13             Munich           GPE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display results as a table\n",
    "print(\"Entity extraction using NLTK's NE Recognizer:\")\n",
    "print(entity_frame)   \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a2d7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the process above for the stanford NE Recognizer (Java program wrapped in NLTK)\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce1b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set java path in environment variables\n",
    "java_path = r'/usr/bin/java'\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "188d63c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-ner.jar jar file at /Users/arw/Documents/Work/stanford-ner-2014-08-27/stanford-ner.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load stanford NER by pointing to the pre-trained English model and the code (jar file)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sn \u001b[38;5;241m=\u001b[39m \u001b[43mStanfordNERTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/arw/Documents/Work/stanford-ner-2014-08-27/classifiers/english.all.3class.distsim.crf.ser.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mpath_to_jar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/arw/Documents/Work/stanford-ner-2014-08-27/stanford-ner.jar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\stanford.py:200\u001b[0m, in \u001b[0;36mStanfordNERTagger.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\stanford.py:70\u001b[0m, in \u001b[0;36mStanfordTagger.__init__\u001b[1;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_JAR:\n\u001b[0;32m     65\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe StanfordTagger class is not meant to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstantiated directly. Did you mean \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStanfordPOSTagger or StanfordNERTagger?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stanford_jar \u001b[38;5;241m=\u001b[39m \u001b[43mfind_jar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_JAR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_to_jar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearchpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stanford_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stanford_model \u001b[38;5;241m=\u001b[39m find_file(\n\u001b[0;32m     75\u001b[0m     model_filename, env_vars\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTANFORD_MODELS\u001b[39m\u001b[38;5;124m\"\u001b[39m,), verbose\u001b[38;5;241m=\u001b[39mverbose\n\u001b[0;32m     76\u001b[0m )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoding \u001b[38;5;241m=\u001b[39m encoding\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\internals.py:848\u001b[0m, in \u001b[0;36mfind_jar\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_jar\u001b[39m(\n\u001b[0;32m    840\u001b[0m     name_pattern,\n\u001b[0;32m    841\u001b[0m     path_to_jar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    846\u001b[0m     is_regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    847\u001b[0m ):\n\u001b[1;32m--> 848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfind_jar_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname_pattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_to_jar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearchpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_regex\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\internals.py:734\u001b[0m, in \u001b[0;36mfind_jar_iter\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    732\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m path_to_jar\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 734\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(\n\u001b[0;32m    735\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname_pattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m jar file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_to_jar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    736\u001b[0m         )\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# Check environment variables\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_var \u001b[38;5;129;01min\u001b[39;00m env_vars:\n",
      "\u001b[1;31mLookupError\u001b[0m: Could not find stanford-ner.jar jar file at /Users/arw/Documents/Work/stanford-ner-2014-08-27/stanford-ner.jar"
     ]
    }
   ],
   "source": [
    "# Load stanford NER by pointing to the pre-trained English model and the code (jar file)\n",
    "sn = StanfordNERTagger('/Users/arw/Documents/Work/stanford-ner-2014-08-27/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                       path_to_jar='/Users/arw/Documents/Work/stanford-ner-2014-08-27/stanford-ner.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ed9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First POS-tag the sentences                       \n",
    "ne_annotated_sentences = [sn.tag(sent) for sent in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b2ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then extract the named entities\n",
    "named_entities = []\n",
    "for sentence in ne_annotated_sentences:\n",
    "    temp_entity_name = ''\n",
    "    temp_named_entity = None\n",
    "    for term, tag in sentence:\n",
    "        # Get terms with NE tags\n",
    "        if tag != 'O':\n",
    "            temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
    "            temp_named_entity = (temp_entity_name, tag)\n",
    "        else:\n",
    "            if temp_named_entity:\n",
    "                named_entities.append(temp_named_entity)\n",
    "                temp_entity_name = ''\n",
    "                temp_named_entity = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f3bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the unique named entities - why?\n",
    "named_entities = list(set(named_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store named entities in a (pandas) data frame as before\n",
    "entity_frame = pd.DataFrame(named_entities, \n",
    "                            columns=['Entity Name', 'Entity Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57195a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results in a table form as before\n",
    "print(\"Entity extraction using StanfordNER's NE Recognizer:\")\n",
    "print(entity_frame)\n",
    "\n",
    "# Compare the results of the 2 NER systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
