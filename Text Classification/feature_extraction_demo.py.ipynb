{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f74fd9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from numpy.linalg import norm\n",
    "from sklearn import metrics\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33179ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_extractor(corpus, ngram_range=(1,1)):\n",
    "    \n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "\n",
    "def tfidf_transformer(bow_matrix):\n",
    "    \n",
    "    transformer = TfidfTransformer(norm='l2',\n",
    "                                   smooth_idf=True,\n",
    "                                   use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    return transformer, tfidf_matrix\n",
    "\n",
    "\n",
    "def tfidf_extractor(corpus, ngram_range=(1,1)):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=1, \n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def tfidf_wtd_avg_word_vectors(words, tfidf_vector, tfidf_vocabulary, model, num_features):\n",
    "    \n",
    "    word_tfidfs = [tfidf_vector[0, tfidf_vocabulary.get(word)] \n",
    "                   if tfidf_vocabulary.get(word) \n",
    "                   else 0 for word in words]    \n",
    "    word_tfidf_map = {word:tfidf_val for word, tfidf_val in zip(words, word_tfidfs)}\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    wts = 0.\n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            word_vector = model.wv[word]\n",
    "            weighted_word_vector = word_tfidf_map[word] * word_vector\n",
    "            wts = wts + word_tfidf_map[word]\n",
    "            feature_vector = np.add(feature_vector, weighted_word_vector)\n",
    "    if wts:\n",
    "        feature_vector = np.divide(feature_vector, wts)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus, tfidf_vectors, \n",
    "                                   tfidf_vocabulary, model, num_features):\n",
    "                                       \n",
    "    docs_tfidfs = [(doc, doc_tfidf) \n",
    "                   for doc, doc_tfidf \n",
    "                   in zip(corpus, tfidf_vectors)]\n",
    "    features = [tfidf_wtd_avg_word_vectors(tokenized_sentence, tfidf, tfidf_vocabulary,\n",
    "                                   model, num_features)\n",
    "                    for tokenized_sentence, tfidf in docs_tfidfs]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e80ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a 'toy' corpus\n",
    "CORPUS = [\n",
    "    'the sky is blue',\n",
    "    'sky is blue and sky is beautiful',\n",
    "    'the beautiful sky is so blue',\n",
    "    'i love blue cheese'\n",
    "]\n",
    "\n",
    "# Use new_doc as test dataset\n",
    "new_doc = ['loving this blue sky today']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05231594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_features(features, feature_names):\n",
    "    df = pd.DataFrame(data=features,\n",
    "                      columns=feature_names)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfece7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 2 0 2 0 0]\n",
      " [0 1 1 0 1 0 1 1 1]\n",
      " [0 0 1 1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Pass CORPUS to the simplest bow extractor\n",
    "bow_vectorizer, bow_features = bow_extractor(CORPUS)\n",
    "features = bow_features.todense() # Since we can't view the default 'sparse matrix'\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35086599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Need to extract the same features from our test data too!\n",
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "new_doc_features = new_doc_features.todense()\n",
    "print(new_doc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcc17128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'beautiful' 'blue' 'cheese' 'is' 'love' 'sky' 'so' 'the']\n"
     ]
    }
   ],
   "source": [
    "# See which words/tokens these counts are for...\n",
    "feature_names = bow_vectorizer.get_feature_names_out()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b9848a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   1     0    1   0    1\n",
      "1    1          1     1       0   2     0    2   0    0\n",
      "2    0          1     1       0   1     0    1   1    1\n",
      "3    0          0     1       1   0     1    0   0    0\n"
     ]
    }
   ],
   "source": [
    "# Print both the feature names and counts together\n",
    "\n",
    "# for the training data\n",
    "display_features(features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23d33d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   0     0    1   0    0\n"
     ]
    }
   ],
   "source": [
    "# for the test data\n",
    "display_features(new_doc_features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0d10f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# Try the same with tf-idf instead of frequency counts\n",
    "\n",
    "# Use the tfidf_transformer function we defined\n",
    "feature_names = bow_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to the dense form to print the values out    \n",
    "tfidf_trans, tdidf_features = tfidf_transformer(bow_features)\n",
    "features = np.round(tdidf_features.todense(), 2)\n",
    "display_features(features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb56ebf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Do the same for the test document\n",
    "nd_tfidf = tfidf_trans.transform(new_doc_features)\n",
    "nd_features = np.round(nd_tfidf.todense(), 2)\n",
    "display_features(nd_features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edced06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tf-idf scores/vectors ourselves from scratch\n",
    "# - without using sklearn's TfidfTransformer class\n",
    "feature_names = bow_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "016c6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute term frequencies by simply using bow model\n",
    "tf = bow_features.todense()\n",
    "tf = np.array(tf, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08388542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love  sky   so  the\n",
      "0  0.0        0.0   1.0     0.0  1.0   0.0  1.0  0.0  1.0\n",
      "1  1.0        1.0   1.0     0.0  2.0   0.0  2.0  0.0  0.0\n",
      "2  0.0        1.0   1.0     0.0  1.0   0.0  1.0  1.0  1.0\n",
      "3  0.0        0.0   1.0     1.0  0.0   1.0  0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# Check if the term frequencies are as expected\n",
    "display_features(tf, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b60f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the document frequency matrix\n",
    "df = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\n",
    "df = 1 + df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25a79a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    2          3     5       2   4     2    4   2    3\n"
     ]
    }
   ],
   "source": [
    "# Check if the document frequencies are as expected\n",
    "display_features([df], feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64ae9afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the inverse document frequencies\n",
    "total_docs = 1 + len(CORPUS)\n",
    "idf = 1.0 + np.log(float(total_docs) / df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e37b8cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  1.92       1.51   1.0    1.92  1.22  1.92  1.22  1.92  1.51\n"
     ]
    }
   ],
   "source": [
    "# Check if the inverse document frequencies are as expected\n",
    "display_features([np.round(idf, 2)], feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01f510ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the idf diagonal matrix  \n",
    "total_features = bow_features.shape[1]\n",
    "idf_diag = sp.spdiags(idf, diags=0, m=total_features, n=total_features)\n",
    "idf = idf_diag.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feb9d98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.92 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   1.51 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   1.92 0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   1.22 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   1.92 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   1.22 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.92 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   1.51]]\n"
     ]
    }
   ],
   "source": [
    "# Is the idf diagonal matrix as expected?\n",
    "print(np.round(idf, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60370660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the full tfidf feature matrix\n",
    "tfidf = tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4e6cd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00   1.0    0.00  1.22  0.00  1.22  0.00  1.51\n",
      "1  1.92       1.51   1.0    0.00  2.45  0.00  2.45  0.00  0.00\n",
      "2  0.00       1.51   1.0    0.00  1.22  0.00  1.22  1.92  1.51\n",
      "3  0.00       0.00   1.0    1.92  0.00  1.92  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# Is the tfidf feature matrix what we expected?\n",
    "display_features(np.round(tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37f61fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the L2 norms\n",
    "norms = norm(tfidf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "278073fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5  4.35 3.5  2.89]\n"
     ]
    }
   ],
   "source": [
    "# Display the L2 norms for each document\n",
    "print(np.round(norms, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5790ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 'normalized' tfidf\n",
    "norm_tfidf = tfidf / norms[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50b372b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# Check if the final tfidf feature matrix is as expected\n",
    "# Is it the same as what we got using the TfidfTransformer class of sklearn?\n",
    "display_features(np.round(norm_tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35bd573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the test data \n",
    "# First, compute the term freqs from bow freqs for the test data - new_doc\n",
    "nd_tf = new_doc_features\n",
    "nd_tf = np.array(nd_tf, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dd16a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next compute tfidf using idf matrix from the train corpus\n",
    "nd_tfidf = nd_tf*idf\n",
    "nd_norms = norm(nd_tfidf, axis=1)\n",
    "norm_nd_tfidf = nd_tfidf / nd_norms[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7866b5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# Check the new_doc tfidf feature vector\n",
    "display_features(np.round(norm_nd_tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "011f8350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# sklearn's TfidfVectorizer provides a transformer to extract tfidf scores directly\n",
    "# from raw data - avoiding the need for CountVectorizer based bow scores\n",
    "tfidf_vectorizer, tdidf_features = tfidf_extractor(CORPUS)\n",
    "display_features(np.round(tdidf_features.todense(), 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fdbb744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "nd_tfidf = tfidf_vectorizer.transform(new_doc)\n",
    "display_features(np.round(nd_tfidf.todense(), 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f83afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do more sophisticated word-vector models using \n",
    "# Google's word2vec algorithm and\n",
    "# the gensim python package\n",
    "TOKENIZED_CORPUS = [nltk.word_tokenize(sentence) \n",
    "                    for sentence in CORPUS]\n",
    "tokenized_new_doc = [nltk.word_tokenize(sentence) \n",
    "                    for sentence in new_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d01e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters for the NN-based word2vec 'word embeddings':\n",
    "\n",
    "model = gensim.models.Word2Vec(TOKENIZED_CORPUS, \n",
    "                               vector_size=10, # dimension of the word vectors (tens to thousands)\n",
    "                               window=10, # window size to conside the context of a word\n",
    "                               min_count=2, # minimum frequency of a word in the whole corpus to be included in vocabulary\n",
    "                               sample=1e-3) # used to downsample the effects of the occurence of frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fe2f154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.011  0.022 -0.004  0.03  -0.02  -0.032  0.002  0.063 -0.04  -0.02 ]\n",
      " [ 0.036  0.011  0.032  0.027 -0.014 -0.032  0.031  0.028 -0.061 -0.059]\n",
      " [-0.006  0.016  0.016  0.009 -0.02  -0.029  0.018  0.039 -0.032 -0.025]\n",
      " [-0.005  0.002  0.051  0.09  -0.093 -0.071  0.065  0.09  -0.05  -0.038]]\n"
     ]
    }
   ],
   "source": [
    "# Averaging word vectors of a document\n",
    "avg_word_vec_features = averaged_word_vectorizer(corpus=TOKENIZED_CORPUS,\n",
    "                                                 model=model,\n",
    "                                                 num_features=10)\n",
    "print(np.round(avg_word_vec_features, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "afd60a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.034  0.027  0.059  0.049 -0.015 -0.053  0.028  0.074 -0.063 -0.038]]\n"
     ]
    }
   ],
   "source": [
    "nd_avg_word_vec_features = averaged_word_vectorizer(corpus=tokenized_new_doc,\n",
    "                                                    model=model,\n",
    "                                                    num_features=10)\n",
    "print(np.round(nd_avg_word_vec_features, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "299a6239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.006  0.024 -0.011  0.023 -0.015 -0.028 -0.006  0.064 -0.034 -0.012]\n",
      " [ 0.049  0.013  0.025  0.027 -0.005 -0.029  0.023  0.027 -0.067 -0.062]\n",
      " [-0.013  0.017  0.014  0.    -0.017 -0.026  0.015  0.035 -0.026 -0.021]\n",
      " [-0.005  0.002  0.051  0.09  -0.093 -0.071  0.065  0.09  -0.05  -0.038]]\n"
     ]
    }
   ],
   "source": [
    "# Using tfidf weighted average of word vectors in a document              \n",
    "corpus_tfidf = tdidf_features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus=TOKENIZED_CORPUS,\n",
    "                                                                     tfidf_vectors=corpus_tfidf,\n",
    "                                                                     tfidf_vocabulary=vocab,\n",
    "                                                                     model=model, \n",
    "                                                                     num_features=10)\n",
    "print(np.round(wt_tfidf_word_vec_features, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6243480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.038  0.029  0.06   0.045 -0.007 -0.051  0.024  0.072 -0.064 -0.039]]\n"
     ]
    }
   ],
   "source": [
    "nd_wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_new_doc,\n",
    "                                                                     tfidf_vectors=nd_tfidf,\n",
    "                                                                     tfidf_vocabulary=vocab,\n",
    "                                                                     model=model, \n",
    "                                                                     num_features=10)\n",
    "print(np.round(nd_wt_tfidf_word_vec_features, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
