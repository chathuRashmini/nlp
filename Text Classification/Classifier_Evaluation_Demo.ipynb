{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe72b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b64168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding Machine Learning Evaluation metrics using a 'confusion matrix'\n",
    "# Use sklearn's metrics method and compare results with what we expect by calculating the metrics manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91447456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some dummy data for the metric methods to compute the metrics\n",
    "actual_labels = ['spam', 'ham', 'spam', 'spam', 'spam',\n",
    "               'ham', 'ham', 'spam', 'ham', 'spam',\n",
    "               'spam', 'ham', 'ham', 'ham', 'spam',\n",
    "               'ham', 'ham', 'spam', 'spam', 'ham']\n",
    "              \n",
    "predicted_labels = ['spam', 'spam', 'spam', 'ham', 'spam',\n",
    "                    'spam', 'ham', 'ham', 'spam', 'spam',\n",
    "                    'ham', 'ham', 'spam', 'ham', 'ham',\n",
    "                    'ham', 'spam', 'ham', 'spam', 'spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4da25ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer the cells of the confusion matrix                    \n",
    "ac = Counter(actual_labels)                     \n",
    "pc = Counter(predicted_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a629e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual counts: [('spam', 10), ('ham', 10)]\n",
      "Predicted counts: [('spam', 11), ('ham', 9)]\n"
     ]
    }
   ],
   "source": [
    "print('Actual counts:', ac.most_common())\n",
    "print('Predicted counts:', pc.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1abc179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define confusion matrix using sklearn function\n",
    "cm = metrics.confusion_matrix(y_true=actual_labels,\n",
    "                         y_pred=predicted_labels,\n",
    "                         labels=['spam','ham'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fceb839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Predicted:    \n",
      "                   spam ham\n",
      "Actual: spam          5   5\n",
      "        ham           6   4\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(data=cm, \n",
    "                   columns=pd.MultiIndex(levels=[['Predicted:'],\n",
    "                                                 ['spam','ham']], \n",
    "                                         codes=[[0,0],[0,1]]), \n",
    "                   index=pd.MultiIndex(levels=[['Actual:'],\n",
    "                                               ['spam','ham']], \n",
    "                                       codes=[[0,0],[0,1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3489f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare what is positive - other will be taken as negative   \n",
    "positive_class = 'spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2b90890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the metric 'accuracy' using metric function\n",
    "accuracy = np.round(\n",
    "                metrics.accuracy_score(y_true=actual_labels,\n",
    "                                       y_pred=predicted_labels),\n",
    "                2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb78ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 'accuracy' metric manually\n",
    "# For this, declare the counts in each cell... manually!\n",
    "true_positive = 5.\n",
    "false_positive = 6.\n",
    "false_negative = 5.\n",
    "true_negative = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "207e25f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.45\n",
      "Manually computed accuracy: 0.45\n"
     ]
    }
   ],
   "source": [
    "accuracy_manual = np.round(\n",
    "                    (true_positive + true_negative) /\n",
    "                      (true_positive + true_negative +\n",
    "                       false_negative + false_positive),2)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Manually computed accuracy:', accuracy_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20678277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.45\n",
      "Manually computed precision: 0.45\n"
     ]
    }
   ],
   "source": [
    "# Calculate 'precision' using the function and compare with our manual calculation\n",
    "precision = np.round(\n",
    "                metrics.precision_score(y_true=actual_labels,\n",
    "                                        y_pred=predicted_labels,\n",
    "                                        pos_label=positive_class),2)\n",
    "precision_manual = np.round(\n",
    "                        (true_positive) /\n",
    "                        (true_positive + false_positive),2)\n",
    "print('Precision:', precision)\n",
    "print('Manually computed precision:', precision_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4219e0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5\n",
      "Manually computed recall: 0.5\n"
     ]
    }
   ],
   "source": [
    "# calculate 'recall' using the function and compare with our manual calculation\n",
    "recall = np.round(\n",
    "            metrics.recall_score(y_true=actual_labels,\n",
    "                                 y_pred=predicted_labels,\n",
    "                                 pos_label=positive_class),2)\n",
    "recall_manual = np.round(\n",
    "                    (true_positive) /\n",
    "                    (true_positive + false_negative),2)\n",
    "print('Recall:', recall)\n",
    "print('Manually computed recall:', recall_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd2e3a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.48\n",
      "Manually computed F1 score: 0.47\n"
     ]
    }
   ],
   "source": [
    "# calculate 'F1-score' using the function and compare with our manual calculation\n",
    "f1_score = np.round(\n",
    "                metrics.f1_score(y_true=actual_labels,\n",
    "                                 y_pred=predicted_labels,\n",
    "                                 pos_label=positive_class),2) \n",
    "f1_score_manual = np.round(\n",
    "                    (2 * precision * recall) /\n",
    "                    (precision + recall),2)\n",
    "print('F1 score:', f1_score)\n",
    "print('Manually computed F1 score:', f1_score_manual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
