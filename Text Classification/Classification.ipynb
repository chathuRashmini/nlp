{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8302f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset comprises of 20k newsposts in 20 different newsgroups - what topics are they on?\n",
    "# So, this will be a 20-class classification problem!\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b196d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest feature extraction is to use bow - i.e. extract counts of unigrams\n",
    "# Instead of a frequency bow, we can also use n-gram bow model\n",
    "# Try to extract trigram bows\n",
    "def bow_extractor(corpus, ngram_range=(1,1)):\n",
    "    \n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "\n",
    "# Weighting each term by how important it is in the document collection\n",
    "def tfidf_transformer(bow_matrix):\n",
    "    \n",
    "    transformer = TfidfTransformer(norm='l2',\n",
    "                                   smooth_idf=True,\n",
    "                                   use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    return transformer, tfidf_matrix\n",
    "\n",
    "\n",
    "# TfidfVectorizer is the powerful equivalent of CountVectorizer for TF-IDF weights\n",
    "def tfidf_extractor(corpus, ngram_range=(1,1)):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=1, \n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "\n",
    "# Two weighting schemes for combining word vectors in documents\n",
    "# 1. averaging the word vectors\n",
    "# 2. tf-idf weighting of word vectors\n",
    "\n",
    "# Averaging word vectors of a document\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# Using tfidf weighted average of word vectors in a document  \n",
    "def tfidf_wtd_avg_word_vectors(words, tfidf_vector, tfidf_vocabulary, model, num_features):\n",
    "    \n",
    "    word_tfidfs = [tfidf_vector[0, tfidf_vocabulary.get(word)] \n",
    "                   if tfidf_vocabulary.get(word) \n",
    "                   else 0 for word in words]    \n",
    "    word_tfidf_map = {word:tfidf_val for word, tfidf_val in zip(words, word_tfidfs)}\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    wts = 0.\n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            word_vector = model.wv[word]\n",
    "            weighted_word_vector = word_tfidf_map[word] * word_vector\n",
    "            wts = wts + word_tfidf_map[word]\n",
    "            feature_vector = np.add(feature_vector, weighted_word_vector)\n",
    "    if wts:\n",
    "        feature_vector = np.divide(feature_vector, wts)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus, tfidf_vectors, \n",
    "                                   tfidf_vocabulary, model, num_features):\n",
    "                                       \n",
    "    docs_tfidfs = [(doc, doc_tfidf) \n",
    "                   for doc, doc_tfidf \n",
    "                   in zip(corpus, tfidf_vectors)]\n",
    "    features = [tfidf_wtd_avg_word_vectors(tokenized_sentence, tfidf, tfidf_vocabulary,\n",
    "                                   model, num_features)\n",
    "                    for tokenized_sentence, tfidf in docs_tfidfs]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c625331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove headers of email since they won't help us in \n",
    "def get_data():\n",
    "    data = fetch_20newsgroups(subset='all',\n",
    "                              shuffle=True,\n",
    "                              remove=('headers', 'footers', 'quotes'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a92505fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into training and testing sets\n",
    "def prepare_datasets(corpus, labels, test_data_proportion=0.3):\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels, test_size=0.33, random_state=42)\n",
    "    return train_X, test_X, train_Y, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58a45c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty documents since they would just add noise\n",
    "def remove_empty_docs(corpus, labels):\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for doc, label in zip(corpus, labels):\n",
    "        if doc.strip():\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "\n",
    "    return filtered_corpus, filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a9e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8598320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58add1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, labels = dataset.data, dataset.target\n",
    "corpus, labels = remove_empty_docs(corpus, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b63694bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document: the blood of the lamb.\n",
      "\n",
      "This will be a hard task, because most cultures used most animals\n",
      "for blood sacrifices. It has to be something related to our current\n",
      "post-modernism state. Hmm, what about used computers?\n",
      "\n",
      "Cheers,\n",
      "Kent\n"
     ]
    }
   ],
   "source": [
    "print('Sample document:', corpus[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "796bda8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 19\n"
     ]
    }
   ],
   "source": [
    "print('Class label:',labels[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a258d994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual class label: talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "print('Actual class label:', dataset.target_names[labels[10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99442220",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus, labels, test_data_proportion=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ad424aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First normalize both the training and test data using our previous funcionts \n",
    "from normalization import normalize_corpus\n",
    "\n",
    "norm_train_corpus = normalize_corpus(train_corpus)\n",
    "norm_test_corpus = normalize_corpus(test_corpus)  \n",
    "\n",
    "''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cf38127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features using the extractors we defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f2b41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words (BoW) features\n",
    "bow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus)  \n",
    "bow_test_features = bow_vectorizer.transform(norm_test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f62505b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF features\n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)\n",
    "tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01c1306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize documents\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                   for text in norm_train_corpus]\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                   for text in norm_test_corpus] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8f2da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word2vec model \n",
    "model = gensim.models.Word2Vec(tokenized_train, vector_size=500, window=100, min_count=30, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78ea68b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaged word vector features\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train, model=model, num_features=500)\n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test, model=model, num_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1310b811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.22770961540702142"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_wv_train_features[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae9dcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf weighted averaged word vector features\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train, \n",
    "                                                                  tfidf_vectors=tfidf_train_features,\n",
    "                                                                  tfidf_vocabulary=vocab, model=model, num_features=500)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_test, tfidf_vectors=tfidf_test_features, \n",
    "                                                                 tfidf_vocabulary=vocab, model=model, num_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82cad9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn's metrics function for evaluation of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3027be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate the 4 common mertics\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print('Accuracy:', np.round(metrics.accuracy_score(true_labels, predicted_labels), 2))\n",
    "    print('Precision:', np.round(metrics.precision_score(true_labels, predicted_labels, average='weighted'), 2))\n",
    "    print('Recall:', np.round(metrics.recall_score(true_labels, predicted_labels, average='weighted'), 2))\n",
    "    print('F1 Score:', np.round(metrics.f1_score(true_labels, predicted_labels, average='weighted'), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0da61d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master function to call the above defined functions to perform the classification,\n",
    "# predict the results and evaluate predictions against the test data\n",
    "def train_predict_evaluate_model(classifier, train_features, train_labels, test_features, test_labels): \n",
    "    # build model    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    \n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features)\n",
    "    \n",
    "    # evaluate model prediction performance \n",
    "    get_metrics(true_labels=test_labels, predicted_labels=predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11f977c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the two classification algorithms we want to use for the task\n",
    "# Based on the features we extract, we altogether have 6 combinations of models to train\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB() # Multinomial Naive Bayes\n",
    "svm = SGDClassifier(loss='hinge', max_iter=100) # Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c470364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67\n",
      "Precision: 0.73\n",
      "Recall: 0.67\n",
      "F1 Score: 0.65\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes with bag of words features\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb, \n",
    "                                                   train_features=bow_train_features, \n",
    "                                                   train_labels=train_labels, \n",
    "                                                   test_features=bow_test_features, \n",
    "                                                   test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54ec70d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.64\n",
      "Precision: 0.68\n",
      "Recall: 0.64\n",
      "F1 Score: 0.65\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with bag of words features\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7feed18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n",
      "Precision: 0.78\n",
      "Recall: 0.72\n",
      "F1 Score: 0.71\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes with tfidf features\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7a6b54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "Precision: 0.77\n",
      "Recall: 0.77\n",
      "F1 Score: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with tfidf features\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd1ce114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.53\n",
      "Precision: 0.57\n",
      "Recall: 0.53\n",
      "F1 Score: 0.51\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with averaged word vector features\n",
    "svm_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2d015c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.53\n",
      "Precision: 0.55\n",
      "Recall: 0.53\n",
      "F1 Score: 0.52\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine with tfidf weighted averaged word vector features\n",
    "svm_tfidfwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_wv_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39dff1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>157</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>225</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>219</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>222</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>226</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>270</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>270</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>246</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>253</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>274</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>282</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>212</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>267</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>265</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>286</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>224</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>259</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>166</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>58</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9    10   11   12   13   14  \\\n",
       "0   157    2    0    1    1    2    2    3    3    0    5    3    3    6    5   \n",
       "1     1  225    9    7    6   16    6    2    2    1    0    3    5    3    4   \n",
       "2     1   19  219   20    7   21    9    1    0    0    0    3    6    2    1   \n",
       "3     1   12   24  222   10    4    9    2    1    1    1    2    6    3    1   \n",
       "4     1    5    8   14  226    4    4    2    3    1    0    3   11    3    4   \n",
       "5     0   21   18    2    1  270    1    0    1    0    0    0    4    4    1   \n",
       "6     0    2    7   10   12    1  270   10    3    2    1    1   10    1    4   \n",
       "7     3    4    2    0    2    4    4  246   21    1    2    1    9    4    2   \n",
       "8     4    0    0    4    3    2    4   25  253    3    2    3    1    5    2   \n",
       "9     2    1    1    0    2    3    5    5    5  274   13    1    2    1    2   \n",
       "10    0    0    0    0    0    1    0    2    2    5  282    1    2    1    3   \n",
       "11    3    6    3    3    1    3    3    2    2    0    0  257    7    3    0   \n",
       "12    0    5    6   15    9    2   16    9    8    3    5    1  212    4    4   \n",
       "13    2    3    0    2    3    3    2    1    2    0    1    1    8  267    4   \n",
       "14    0    5    1    0    2    4    2    4    3    1    1    0    8    4  265   \n",
       "15   13    1    0    1    1    1    0    1    4    1    1    2    1    7    5   \n",
       "16    3    1    0    0    0    4    2    1    7    4    2   10    3    3    5   \n",
       "17    6    0    1    0    1    3    0    2    3    2    5    5    1    3    1   \n",
       "18   10    1    2    1    0    1    3    1    6    3    3    9    0    8    5   \n",
       "19   18    4    1    1    0    2    4    2    6    3    1    1    0   10    3   \n",
       "\n",
       "     15   16   17   18  19  \n",
       "0    31    4    7    8  20  \n",
       "1     2    2    0    3   0  \n",
       "2     2    0    1    2   0  \n",
       "3     0    1    0    0   0  \n",
       "4     1    1    0    1   0  \n",
       "5     1    0    1    0   0  \n",
       "6     0    2    2    1   0  \n",
       "7     2    2    3    4   0  \n",
       "8     3    1    2    5   0  \n",
       "9     3    2    1    2   0  \n",
       "10    1    0    1    2   0  \n",
       "11    1    4    2    6   0  \n",
       "12    0    0    1    1   1  \n",
       "13    2    3    0    4   0  \n",
       "14    2    5    2    4   1  \n",
       "15  286    4    5    2   7  \n",
       "16    2  224    5   13   2  \n",
       "17    6    5  259   10   2  \n",
       "18    4   30    6  166   3  \n",
       "19   58   20    8    6  65  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix for the tfidf-based SVM model (best in this case?)\n",
    "import pandas as pd\n",
    "cm = metrics.confusion_matrix(test_labels, svm_tfidf_predictions)\n",
    "pd.DataFrame(cm, index=range(0,20), columns=range(0,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10d1678a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism -> soc.religion.christian\n",
      "talk.politics.misc -> talk.politics.guns\n",
      "talk.religion.misc -> soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "class_names = dataset.target_names\n",
    "print(class_names[0], '->', class_names[15])\n",
    "print(class_names[18], '->', class_names[16]) \n",
    "print(class_names[19], '->', class_names[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5604704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: alt.atheism\n",
      "Predicted Label: soc.religion.christian\n",
      "Document:-\n",
      "I would like a list of Bible contadictions from those of you who dispite being free from Christianity are well versed in the Bible. \n",
      "\n",
      "Actual Label: alt.atheism\n",
      "Predicted Label: soc.religion.christian\n",
      "Document:-\n",
      "  They spent quite a bit of time on the wording of the Constitution.  They picked words whose meanings implied the intent.  We have already looked in the dictionary to define the word.  Isn't this sufficient?   But we were discussing it in relation to the death penalty.  And, the Constitution need not define each of the words within.  Anyone who doesn't know what cruel is can look in the dictionary (and we did).\n",
      "\n",
      "Actual Label: alt.atheism\n",
      "Predicted Label: soc.religion.christian\n",
      "Document:-\n",
      "Our Lord and Savior David Keresh has risen!   \tHe has been seen alive!   \tSpread the word!     --------------------------------------------------------------------------------\n",
      "\n",
      "Actual Label: alt.atheism\n",
      "Predicted Label: soc.religion.christian\n",
      "Document:-\n",
      "  \"This is your god\" (from John Carpenter's \"They Live,\" natch)  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the misclassified documents for error analysis\n",
    "import re\n",
    "\n",
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label == 0 and predicted_label == 15:\n",
    "        print('Actual Label:', class_names[label])\n",
    "        print('Predicted Label:', class_names[predicted_label])\n",
    "        print('Document:-')\n",
    "        print(re.sub('\\n', ' ', document))\n",
    "        print()\n",
    "        \n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b59ccbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: talk.politics.misc\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "After the initial gun battle was over, they had 50 days to come out peacefully. They had their high priced lawyer, and judging by the posts here they had some public support. Can anyone come up with a rational explanation why the didn't come out (even after they negotiated coming out after the radio sermon) that doesn't include the Davidians wanting to commit suicide/murder/general mayhem?\n",
      "\n",
      "Actual Label: talk.politics.misc\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "Yesterday, the FBI was saying that at least three of the bodies had gunshot wounds, indicating that they were shot trying to escape the fire.  Today's paper quotes the medical examiner as saying that there is no evidence of gunshot wounds in any of the recovered bodies.  At the beginning of this siege, it was reported that while Koresh had a class III (machine gun) license, today's paper quotes the government as saying, no, they didn't have a license.  Today's paper reports that a number of the bodies were found with shoulder weapons next to them, as if they had been using them while dying -- which doesn't sound like the sort of action I would expect from a suicide.  Our government lies, as it tries to cover over its incompetence and negligence.  Why should I believe the FBI's claims about anything else, when we can see that they are LYING?  This system of government is beyond reform.\n",
      "\n",
      "Actual Label: talk.politics.misc\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "  Well, for one thing most, if not all the Dividians (depending on whether they could show they acted in self-defense and there were no illegal weapons), could have gone on with their life as they were living it. No one was forcing them to give up their religion or even their legal weapons. The Dividians had survived a change in leadership before so even if Koresch himself would have been convicted and sent to jail, they still could have carried on.   I don't think the Dividians were insane, but I don't see a reason for mass suicide (if the fire was intentional set by some of the Dividians.) We also don't know that, if the fire was intentionally set from inside, was it a generally know plan or was this something only an inner circle knew about, or was it something two or three felt they had to do with or without Koresch's knowledge/blessing, etc.? I don't know much about Masada. Were some people throwing others over? Did mothers jump over with their babies in their arms?\n",
      "\n",
      "Actual Label: talk.politics.misc\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "   True.  Today's Boston Globe interviewed a former Unification Church   leader who is now a consultant on cults.  He said the FBI's approach   was totally wrong.  He said they should have tried to break down the    BD's loyalty to Koresh through psychological means.   Koresh's whole   theology was based on an approaching confrontation with the forces    of evil in the world and a seige mentality based on this.  The Feds   played into his hands **PERFECTLY**.   By surrounding the compound   with tanks and playing loud rock music and glaring lights at them    they strongly reinforced Koresh's message that the outside world was   evil and threatening.    He said instead they should have set up    a picnic atmosphere, and acted inviting and friendly.  If they   broadcast anything over PA systems it should have been loving    relatives reflecting on pleasant events from the cult members'   childhoods.   The idea is to make the outside world and surrender   seem like a pleasant, desirable alternative.   Interesting comments. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label == 18 and predicted_label == 16:\n",
    "        print('Actual Label:', class_names[label])\n",
    "        print('Predicted Label:', class_names[predicted_label])\n",
    "        print('Document:-')\n",
    "        print(re.sub('\\n', ' ', document))\n",
    "        print()\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
