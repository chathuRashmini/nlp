{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d61effc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd76fe48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8d23895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call', 'me', 'Ishmael', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 \n",
    "# a list of tokens of the first sentence of each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c8fcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching Text ------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "751e5ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 11 of 11 matches:\n",
      "ong the former , one was of a most monstrous size . ... This came towards us , \n",
      "ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\n",
      "ll over with a heathenish array of monstrous clubs and spears . Some were thick\n",
      "d as you gazed , and wondered what monstrous cannibal and savage could ever hav\n",
      "that has survived the flood ; most monstrous and most mountainous ! That Himmal\n",
      "they might scout at Moby Dick as a monstrous fable , or still worse and more de\n",
      "th of Radney .'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l\n",
      "ing Scenes . In connexion with the monstrous pictures of whales , I am strongly\n",
      "ere to enter upon those still more monstrous stories of them which are to be fo\n",
      "ght have been rummaged out of this monstrous cabinet there is no telling . But \n",
      "of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n"
     ]
    }
   ],
   "source": [
    "text1.concordance(\"monstrous\")\n",
    "# search for any word that you give to the function and show you the occurrences and some surrounding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "107305cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true contemptible christian abundant few part mean careful puzzled\n",
      "mystifying passing curious loving wise doleful gamesome singular\n",
      "delightfully perilous fearless\n"
     ]
    }
   ],
   "source": [
    "text1.similar(\"monstrous\")\n",
    "# finds all the words that are used in the same context as the one given, where the context is the word before and the word after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a3bb298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting Vocabulary ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a9b0e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44764"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2470d924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " ',)',\n",
       " '.',\n",
       " '.)',\n",
       " ':',\n",
       " ';',\n",
       " ';)',\n",
       " '?',\n",
       " '?)',\n",
       " 'A',\n",
       " 'Abel',\n",
       " 'Abelmizraim',\n",
       " 'Abidah',\n",
       " 'Abide',\n",
       " 'Abimael',\n",
       " 'Abimelech',\n",
       " 'Abr',\n",
       " 'Abrah',\n",
       " 'Abraham',\n",
       " 'Abram',\n",
       " 'Accad',\n",
       " 'Achbor',\n",
       " 'Adah',\n",
       " 'Adam',\n",
       " 'Adbeel',\n",
       " 'Admah',\n",
       " 'Adullamite',\n",
       " 'After',\n",
       " 'Aholibamah',\n",
       " 'Ahuzzath',\n",
       " 'Ajah',\n",
       " 'Akan',\n",
       " 'All',\n",
       " 'Allonbachuth',\n",
       " 'Almighty',\n",
       " 'Almodad',\n",
       " 'Also',\n",
       " 'Alvah',\n",
       " 'Alvan',\n",
       " 'Am',\n",
       " 'Amal',\n",
       " 'Amalek',\n",
       " 'Amalekites',\n",
       " 'Ammon',\n",
       " 'Amorite',\n",
       " 'Amorites',\n",
       " 'Amraphel',\n",
       " 'An',\n",
       " 'Anah',\n",
       " 'Anamim',\n",
       " 'And',\n",
       " 'Aner',\n",
       " 'Angel',\n",
       " 'Appoint',\n",
       " 'Aram',\n",
       " 'Aran',\n",
       " 'Ararat',\n",
       " 'Arbah',\n",
       " 'Ard',\n",
       " 'Are',\n",
       " 'Areli',\n",
       " 'Arioch',\n",
       " 'Arise',\n",
       " 'Arkite',\n",
       " 'Arodi',\n",
       " 'Arphaxad',\n",
       " 'Art',\n",
       " 'Arvadite',\n",
       " 'As',\n",
       " 'Asenath',\n",
       " 'Ashbel',\n",
       " 'Asher',\n",
       " 'Ashkenaz',\n",
       " 'Ashteroth',\n",
       " 'Ask',\n",
       " 'Asshur',\n",
       " 'Asshurim',\n",
       " 'Assyr',\n",
       " 'Assyria',\n",
       " 'At',\n",
       " 'Atad',\n",
       " 'Avith',\n",
       " 'Baalhanan',\n",
       " 'Babel',\n",
       " 'Bashemath',\n",
       " 'Be',\n",
       " 'Because',\n",
       " 'Becher',\n",
       " 'Bedad',\n",
       " 'Beeri',\n",
       " 'Beerlahairoi',\n",
       " 'Beersheba',\n",
       " 'Behold',\n",
       " 'Bela',\n",
       " 'Belah',\n",
       " 'Benam',\n",
       " 'Benjamin',\n",
       " 'Beno',\n",
       " 'Beor',\n",
       " 'Bera',\n",
       " 'Bered',\n",
       " 'Beriah',\n",
       " 'Bethel',\n",
       " 'Bethlehem',\n",
       " 'Bethuel',\n",
       " 'Beware',\n",
       " 'Bilhah',\n",
       " 'Bilhan',\n",
       " 'Binding',\n",
       " 'Birsha',\n",
       " 'Bless',\n",
       " 'Blessed',\n",
       " 'Both',\n",
       " 'Bow',\n",
       " 'Bozrah',\n",
       " 'Bring',\n",
       " 'But',\n",
       " 'Buz',\n",
       " 'By',\n",
       " 'Cain',\n",
       " 'Cainan',\n",
       " 'Calah',\n",
       " 'Calneh',\n",
       " 'Can',\n",
       " 'Cana',\n",
       " 'Canaan',\n",
       " 'Canaanite',\n",
       " 'Canaanites',\n",
       " 'Canaanitish',\n",
       " 'Caphtorim',\n",
       " 'Carmi',\n",
       " 'Casluhim',\n",
       " 'Cast',\n",
       " 'Cause',\n",
       " 'Chaldees',\n",
       " 'Chedorlaomer',\n",
       " 'Cheran',\n",
       " 'Cherubims',\n",
       " 'Chesed',\n",
       " 'Chezib',\n",
       " 'Come',\n",
       " 'Cursed',\n",
       " 'Cush',\n",
       " 'Damascus',\n",
       " 'Dan',\n",
       " 'Day',\n",
       " 'Deborah',\n",
       " 'Dedan',\n",
       " 'Deliver',\n",
       " 'Diklah',\n",
       " 'Din',\n",
       " 'Dinah',\n",
       " 'Dinhabah',\n",
       " 'Discern',\n",
       " 'Dishan',\n",
       " 'Dishon',\n",
       " 'Do',\n",
       " 'Dodanim',\n",
       " 'Dothan',\n",
       " 'Drink',\n",
       " 'Duke',\n",
       " 'Dumah',\n",
       " 'Earth',\n",
       " 'Ebal',\n",
       " 'Eber',\n",
       " 'Edar',\n",
       " 'Eden',\n",
       " 'Edom',\n",
       " 'Edomites',\n",
       " 'Egy',\n",
       " 'Egypt',\n",
       " 'Egyptia',\n",
       " 'Egyptian',\n",
       " 'Egyptians',\n",
       " 'Ehi',\n",
       " 'Elah',\n",
       " 'Elam',\n",
       " 'Elbethel',\n",
       " 'Eldaah',\n",
       " 'EleloheIsrael',\n",
       " 'Eliezer',\n",
       " 'Eliphaz',\n",
       " 'Elishah',\n",
       " 'Ellasar',\n",
       " 'Elon',\n",
       " 'Elparan',\n",
       " 'Emins',\n",
       " 'En',\n",
       " 'Enmishpat',\n",
       " 'Eno',\n",
       " 'Enoch',\n",
       " 'Enos',\n",
       " 'Ephah',\n",
       " 'Epher',\n",
       " 'Ephra',\n",
       " 'Ephraim',\n",
       " 'Ephrath',\n",
       " 'Ephron',\n",
       " 'Er',\n",
       " 'Erech',\n",
       " 'Eri',\n",
       " 'Es',\n",
       " 'Esau',\n",
       " 'Escape',\n",
       " 'Esek',\n",
       " 'Eshban',\n",
       " 'Eshcol',\n",
       " 'Ethiopia',\n",
       " 'Euphrat',\n",
       " 'Euphrates',\n",
       " 'Eve',\n",
       " 'Even',\n",
       " 'Every',\n",
       " 'Except',\n",
       " 'Ezbon',\n",
       " 'Ezer',\n",
       " 'Fear',\n",
       " 'Feed',\n",
       " 'Fifteen',\n",
       " 'Fill',\n",
       " 'For',\n",
       " 'Forasmuch',\n",
       " 'Forgive',\n",
       " 'From',\n",
       " 'Fulfil',\n",
       " 'G',\n",
       " 'Gad',\n",
       " 'Gaham',\n",
       " 'Galeed',\n",
       " 'Gatam',\n",
       " 'Gather',\n",
       " 'Gaza',\n",
       " 'Gentiles',\n",
       " 'Gera',\n",
       " 'Gerar',\n",
       " 'Gershon',\n",
       " 'Get',\n",
       " 'Gether',\n",
       " 'Gihon',\n",
       " 'Gilead',\n",
       " 'Girgashites',\n",
       " 'Girgasite',\n",
       " 'Give',\n",
       " 'Go',\n",
       " 'God',\n",
       " 'Gomer',\n",
       " 'Gomorrah',\n",
       " 'Goshen',\n",
       " 'Guni',\n",
       " 'Hadad',\n",
       " 'Hadar',\n",
       " 'Hadoram',\n",
       " 'Hagar',\n",
       " 'Haggi',\n",
       " 'Hai',\n",
       " 'Ham',\n",
       " 'Hamathite',\n",
       " 'Hamor',\n",
       " 'Hamul',\n",
       " 'Hanoch',\n",
       " 'Happy',\n",
       " 'Haran',\n",
       " 'Hast',\n",
       " 'Haste',\n",
       " 'Have',\n",
       " 'Havilah',\n",
       " 'Hazarmaveth',\n",
       " 'Hazezontamar',\n",
       " 'Hazo',\n",
       " 'He',\n",
       " 'Hear',\n",
       " 'Heaven',\n",
       " 'Heber',\n",
       " 'Hebrew',\n",
       " 'Hebrews',\n",
       " 'Hebron',\n",
       " 'Hemam',\n",
       " 'Hemdan',\n",
       " 'Here',\n",
       " 'Hereby',\n",
       " 'Heth',\n",
       " 'Hezron',\n",
       " 'Hiddekel',\n",
       " 'Hinder',\n",
       " 'Hirah',\n",
       " 'His',\n",
       " 'Hitti',\n",
       " 'Hittite',\n",
       " 'Hittites',\n",
       " 'Hivite',\n",
       " 'Hobah',\n",
       " 'Hori',\n",
       " 'Horite',\n",
       " 'Horites',\n",
       " 'How',\n",
       " 'Hul',\n",
       " 'Huppim',\n",
       " 'Husham',\n",
       " 'Hushim',\n",
       " 'Huz',\n",
       " 'I',\n",
       " 'If',\n",
       " 'In',\n",
       " 'Irad',\n",
       " 'Iram',\n",
       " 'Is',\n",
       " 'Isa',\n",
       " 'Isaac',\n",
       " 'Iscah',\n",
       " 'Ishbak',\n",
       " 'Ishmael',\n",
       " 'Ishmeelites',\n",
       " 'Ishuah',\n",
       " 'Isra',\n",
       " 'Israel',\n",
       " 'Issachar',\n",
       " 'Isui',\n",
       " 'It',\n",
       " 'Ithran',\n",
       " 'Jaalam',\n",
       " 'Jabal',\n",
       " 'Jabbok',\n",
       " 'Jac',\n",
       " 'Jachin',\n",
       " 'Jacob',\n",
       " 'Jahleel',\n",
       " 'Jahzeel',\n",
       " 'Jamin',\n",
       " 'Japhe',\n",
       " 'Japheth',\n",
       " 'Jared',\n",
       " 'Javan',\n",
       " 'Jebusite',\n",
       " 'Jebusites',\n",
       " 'Jegarsahadutha',\n",
       " 'Jehovahjireh',\n",
       " 'Jemuel',\n",
       " 'Jerah',\n",
       " 'Jetheth',\n",
       " 'Jetur',\n",
       " 'Jeush',\n",
       " 'Jezer',\n",
       " 'Jidlaph',\n",
       " 'Jimnah',\n",
       " 'Job',\n",
       " 'Jobab',\n",
       " 'Jokshan',\n",
       " 'Joktan',\n",
       " 'Jordan',\n",
       " 'Joseph',\n",
       " 'Jubal',\n",
       " 'Judah',\n",
       " 'Judge',\n",
       " 'Judith',\n",
       " 'Kadesh',\n",
       " 'Kadmonites',\n",
       " 'Karnaim',\n",
       " 'Kedar',\n",
       " 'Kedemah',\n",
       " 'Kemuel',\n",
       " 'Kenaz',\n",
       " 'Kenites',\n",
       " 'Kenizzites',\n",
       " 'Keturah',\n",
       " 'Kiriathaim',\n",
       " 'Kirjatharba',\n",
       " 'Kittim',\n",
       " 'Know',\n",
       " 'Kohath',\n",
       " 'Kor',\n",
       " 'Korah',\n",
       " 'LO',\n",
       " 'LORD',\n",
       " 'Laban',\n",
       " 'Lahairoi',\n",
       " 'Lamech',\n",
       " 'Lasha',\n",
       " 'Lay',\n",
       " 'Leah',\n",
       " 'Lehabim',\n",
       " 'Lest',\n",
       " 'Let',\n",
       " 'Letushim',\n",
       " 'Leummim',\n",
       " 'Levi',\n",
       " 'Lie',\n",
       " 'Lift',\n",
       " 'Lo',\n",
       " 'Look',\n",
       " 'Lot',\n",
       " 'Lotan',\n",
       " 'Lud',\n",
       " 'Ludim',\n",
       " 'Luz',\n",
       " 'Maachah',\n",
       " 'Machir',\n",
       " 'Machpelah',\n",
       " 'Madai',\n",
       " 'Magdiel',\n",
       " 'Magog',\n",
       " 'Mahalaleel',\n",
       " 'Mahalath',\n",
       " 'Mahanaim',\n",
       " 'Make',\n",
       " 'Malchiel',\n",
       " 'Male',\n",
       " 'Mam',\n",
       " 'Mamre',\n",
       " 'Man',\n",
       " 'Manahath',\n",
       " 'Manass',\n",
       " 'Manasseh',\n",
       " 'Mash',\n",
       " 'Masrekah',\n",
       " 'Massa',\n",
       " 'Matred',\n",
       " 'Me',\n",
       " 'Medan',\n",
       " 'Mehetabel',\n",
       " 'Mehujael',\n",
       " 'Melchizedek',\n",
       " 'Merari',\n",
       " 'Mesha',\n",
       " 'Meshech',\n",
       " 'Mesopotamia',\n",
       " 'Methusa',\n",
       " 'Methusael',\n",
       " 'Methuselah',\n",
       " 'Mezahab',\n",
       " 'Mibsam',\n",
       " 'Mibzar',\n",
       " 'Midian',\n",
       " 'Midianites',\n",
       " 'Milcah',\n",
       " 'Mishma',\n",
       " 'Mizpah',\n",
       " 'Mizraim',\n",
       " 'Mizz',\n",
       " 'Moab',\n",
       " 'Moabites',\n",
       " 'Moreh',\n",
       " 'Moreover',\n",
       " 'Moriah',\n",
       " 'Muppim',\n",
       " 'My',\n",
       " 'Naamah',\n",
       " 'Naaman',\n",
       " 'Nahath',\n",
       " 'Nahor',\n",
       " 'Naphish',\n",
       " 'Naphtali',\n",
       " 'Naphtuhim',\n",
       " 'Nay',\n",
       " 'Nebajoth',\n",
       " 'Neither',\n",
       " 'Night',\n",
       " 'Nimrod',\n",
       " 'Nineveh',\n",
       " 'Noah',\n",
       " 'Nod',\n",
       " 'Not',\n",
       " 'Now',\n",
       " 'O',\n",
       " 'Obal',\n",
       " 'Of',\n",
       " 'Oh',\n",
       " 'Ohad',\n",
       " 'Omar',\n",
       " 'On',\n",
       " 'Onam',\n",
       " 'Onan',\n",
       " 'Only',\n",
       " 'Ophir',\n",
       " 'Our',\n",
       " 'Out',\n",
       " 'Padan',\n",
       " 'Padanaram',\n",
       " 'Paran',\n",
       " 'Pass',\n",
       " 'Pathrusim',\n",
       " 'Pau',\n",
       " 'Peace',\n",
       " 'Peleg',\n",
       " 'Peniel',\n",
       " 'Penuel',\n",
       " 'Peradventure',\n",
       " 'Perizzit',\n",
       " 'Perizzite',\n",
       " 'Perizzites',\n",
       " 'Phallu',\n",
       " 'Phara',\n",
       " 'Pharaoh',\n",
       " 'Pharez',\n",
       " 'Phichol',\n",
       " 'Philistim',\n",
       " 'Philistines',\n",
       " 'Phut',\n",
       " 'Phuvah',\n",
       " 'Pildash',\n",
       " 'Pinon',\n",
       " 'Pison',\n",
       " 'Potiphar',\n",
       " 'Potipherah',\n",
       " 'Put',\n",
       " 'Raamah',\n",
       " 'Rachel',\n",
       " 'Rameses',\n",
       " 'Rebek',\n",
       " 'Rebekah',\n",
       " 'Rehoboth',\n",
       " 'Remain',\n",
       " 'Rephaims',\n",
       " 'Resen',\n",
       " 'Return',\n",
       " 'Reu',\n",
       " 'Reub',\n",
       " 'Reuben',\n",
       " 'Reuel',\n",
       " 'Reumah',\n",
       " 'Riphath',\n",
       " 'Rosh',\n",
       " 'Sabtah',\n",
       " 'Sabtech',\n",
       " 'Said',\n",
       " 'Salah',\n",
       " 'Salem',\n",
       " 'Samlah',\n",
       " 'Sarah',\n",
       " 'Sarai',\n",
       " 'Saul',\n",
       " 'Save',\n",
       " 'Say',\n",
       " 'Se',\n",
       " 'Seba',\n",
       " 'See',\n",
       " 'Seeing',\n",
       " 'Seir',\n",
       " 'Sell',\n",
       " 'Send',\n",
       " 'Sephar',\n",
       " 'Serah',\n",
       " 'Sered',\n",
       " 'Serug',\n",
       " 'Set',\n",
       " 'Seth',\n",
       " 'Shalem',\n",
       " 'Shall',\n",
       " 'Shalt',\n",
       " 'Shammah',\n",
       " 'Shaul',\n",
       " 'Shaveh',\n",
       " 'She',\n",
       " 'Sheba',\n",
       " 'Shebah',\n",
       " 'Shechem',\n",
       " 'Shed',\n",
       " 'Shel',\n",
       " 'Shelah',\n",
       " 'Sheleph',\n",
       " 'Shem',\n",
       " 'Shemeber',\n",
       " 'Shepho',\n",
       " 'Shillem',\n",
       " 'Shiloh',\n",
       " 'Shimron',\n",
       " 'Shinab',\n",
       " 'Shinar',\n",
       " 'Shobal',\n",
       " 'Should',\n",
       " 'Shuah',\n",
       " 'Shuni',\n",
       " 'Shur',\n",
       " 'Sichem',\n",
       " 'Siddim',\n",
       " 'Sidon',\n",
       " 'Simeon',\n",
       " 'Sinite',\n",
       " 'Sitnah',\n",
       " 'Slay',\n",
       " 'So',\n",
       " 'Sod',\n",
       " 'Sodom',\n",
       " 'Sojourn',\n",
       " 'Some',\n",
       " 'Spake',\n",
       " 'Speak',\n",
       " 'Spirit',\n",
       " 'Stand',\n",
       " 'Succoth',\n",
       " 'Surely',\n",
       " 'Swear',\n",
       " 'Syrian',\n",
       " 'Take',\n",
       " 'Tamar',\n",
       " 'Tarshish',\n",
       " 'Tebah',\n",
       " 'Tell',\n",
       " 'Tema',\n",
       " 'Teman',\n",
       " 'Temani',\n",
       " 'Terah',\n",
       " 'Thahash',\n",
       " 'That',\n",
       " 'The',\n",
       " 'Then',\n",
       " 'There',\n",
       " 'Therefore',\n",
       " 'These',\n",
       " 'They',\n",
       " 'Thirty',\n",
       " 'This',\n",
       " 'Thorns',\n",
       " 'Thou',\n",
       " 'Thus',\n",
       " 'Thy',\n",
       " 'Tidal',\n",
       " 'Timna',\n",
       " 'Timnah',\n",
       " 'Timnath',\n",
       " 'Tiras',\n",
       " 'To',\n",
       " 'Togarmah',\n",
       " 'Tola',\n",
       " 'Tubal',\n",
       " 'Tubalcain',\n",
       " 'Twelve',\n",
       " 'Two',\n",
       " 'Unstable',\n",
       " 'Until',\n",
       " 'Unto',\n",
       " 'Up',\n",
       " 'Upon',\n",
       " 'Ur',\n",
       " 'Uz',\n",
       " 'Uzal',\n",
       " 'We',\n",
       " 'What',\n",
       " 'When',\n",
       " 'Whence',\n",
       " 'Where',\n",
       " 'Whereas',\n",
       " 'Wherefore',\n",
       " 'Which',\n",
       " 'While',\n",
       " 'Who',\n",
       " 'Whose',\n",
       " 'Whoso',\n",
       " 'Why',\n",
       " 'Wilt',\n",
       " 'With',\n",
       " 'Woman',\n",
       " 'Ye',\n",
       " 'Yea',\n",
       " 'Yet',\n",
       " 'Zaavan',\n",
       " 'Zaphnathpaaneah',\n",
       " 'Zar',\n",
       " 'Zarah',\n",
       " 'Zeboiim',\n",
       " 'Zeboim',\n",
       " 'Zebul',\n",
       " 'Zebulun',\n",
       " 'Zemarite',\n",
       " 'Zepho',\n",
       " 'Zerah',\n",
       " 'Zibeon',\n",
       " 'Zidon',\n",
       " 'Zillah',\n",
       " 'Zilpah',\n",
       " 'Zimran',\n",
       " 'Ziphion',\n",
       " 'Zo',\n",
       " 'Zoar',\n",
       " 'Zohar',\n",
       " 'Zuzims',\n",
       " 'a',\n",
       " 'abated',\n",
       " 'abide',\n",
       " 'able',\n",
       " 'abode',\n",
       " 'abomination',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abroad',\n",
       " 'absent',\n",
       " 'abundantly',\n",
       " 'accept',\n",
       " 'accepted',\n",
       " 'according',\n",
       " 'acknowledged',\n",
       " 'activity',\n",
       " 'add',\n",
       " 'adder',\n",
       " 'afar',\n",
       " 'afflict',\n",
       " 'affliction',\n",
       " 'afraid',\n",
       " 'after',\n",
       " 'afterward',\n",
       " 'afterwards',\n",
       " 'aga',\n",
       " 'again',\n",
       " 'against',\n",
       " 'age',\n",
       " 'aileth',\n",
       " 'air',\n",
       " 'al',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'almon',\n",
       " 'alo',\n",
       " 'alone',\n",
       " 'aloud',\n",
       " 'also',\n",
       " 'altar',\n",
       " 'altogether',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'angel',\n",
       " 'angels',\n",
       " 'anger',\n",
       " 'angry',\n",
       " 'anguish',\n",
       " 'anointedst',\n",
       " 'anoth',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'any',\n",
       " 'anything',\n",
       " 'appe',\n",
       " 'appear',\n",
       " 'appeared',\n",
       " 'appease',\n",
       " 'appoint',\n",
       " 'appointed',\n",
       " 'aprons',\n",
       " 'archer',\n",
       " 'archers',\n",
       " 'are',\n",
       " 'arise',\n",
       " 'ark',\n",
       " 'armed',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'arose',\n",
       " 'arrayed',\n",
       " 'art',\n",
       " 'artificer',\n",
       " 'as',\n",
       " 'ascending',\n",
       " 'ash',\n",
       " 'ashamed',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asketh',\n",
       " 'ass',\n",
       " 'assembly',\n",
       " 'asses',\n",
       " 'assigned',\n",
       " 'asswaged',\n",
       " 'at',\n",
       " 'attained',\n",
       " 'audience',\n",
       " 'avenged',\n",
       " 'aw',\n",
       " 'awaked',\n",
       " 'away',\n",
       " 'awoke',\n",
       " 'back',\n",
       " 'backward',\n",
       " 'bad',\n",
       " 'bade',\n",
       " 'badest',\n",
       " 'badne',\n",
       " 'bak',\n",
       " 'bake',\n",
       " 'bakemeats',\n",
       " 'baker',\n",
       " 'bakers',\n",
       " 'balm',\n",
       " 'bands',\n",
       " 'bank',\n",
       " 'bare',\n",
       " 'barr',\n",
       " 'barren',\n",
       " 'basket',\n",
       " 'baskets',\n",
       " 'battle',\n",
       " 'bdellium',\n",
       " 'be',\n",
       " 'bear',\n",
       " 'beari',\n",
       " 'bearing',\n",
       " 'beast',\n",
       " 'beasts',\n",
       " 'beautiful',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'bed',\n",
       " 'been',\n",
       " 'befall',\n",
       " 'befell',\n",
       " 'before',\n",
       " 'began',\n",
       " 'begat',\n",
       " 'beget',\n",
       " 'begettest',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'begotten',\n",
       " 'beguiled',\n",
       " 'beheld',\n",
       " 'behind',\n",
       " 'behold',\n",
       " 'being',\n",
       " 'believed',\n",
       " 'belly',\n",
       " 'belong',\n",
       " 'beneath',\n",
       " 'bereaved',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'besought',\n",
       " 'best',\n",
       " 'betimes',\n",
       " 'better',\n",
       " 'between',\n",
       " 'betwixt',\n",
       " 'beyond',\n",
       " 'binding',\n",
       " 'bird',\n",
       " 'birds',\n",
       " 'birthday',\n",
       " 'birthright',\n",
       " 'biteth',\n",
       " 'bitter',\n",
       " 'blame',\n",
       " 'blameless',\n",
       " 'blasted',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blesseth',\n",
       " 'blessi',\n",
       " 'blessing',\n",
       " 'blessings',\n",
       " 'blindness',\n",
       " 'blood',\n",
       " 'blossoms',\n",
       " 'bodies',\n",
       " 'boldly',\n",
       " 'bondman',\n",
       " 'bondmen',\n",
       " 'bondwoman',\n",
       " 'bone',\n",
       " 'bones',\n",
       " 'book',\n",
       " 'booths',\n",
       " 'border',\n",
       " 'borders',\n",
       " 'born',\n",
       " 'bosom',\n",
       " 'both',\n",
       " 'bottle',\n",
       " 'bou',\n",
       " 'boug',\n",
       " 'bough',\n",
       " 'bought',\n",
       " 'bound',\n",
       " 'bow',\n",
       " 'bowed',\n",
       " 'bowels',\n",
       " 'bowing',\n",
       " 'boys',\n",
       " 'bracelets',\n",
       " 'branches',\n",
       " 'brass',\n",
       " 'bre',\n",
       " 'breach',\n",
       " 'bread',\n",
       " 'breadth',\n",
       " 'break',\n",
       " 'breaketh',\n",
       " 'breaking',\n",
       " 'breasts',\n",
       " 'breath',\n",
       " 'breathed',\n",
       " 'breed',\n",
       " 'brethren',\n",
       " 'brick',\n",
       " 'brimstone',\n",
       " 'bring',\n",
       " 'brink',\n",
       " 'broken',\n",
       " 'brook',\n",
       " 'broth',\n",
       " 'brother',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'bruise',\n",
       " 'budded',\n",
       " 'build',\n",
       " 'builded',\n",
       " 'built',\n",
       " 'bulls',\n",
       " 'bundle',\n",
       " 'bundles',\n",
       " 'burdens',\n",
       " 'buried',\n",
       " 'burn',\n",
       " 'burning',\n",
       " 'burnt',\n",
       " 'bury',\n",
       " 'buryingplace',\n",
       " 'business',\n",
       " 'but',\n",
       " 'butler',\n",
       " 'butlers',\n",
       " 'butlership',\n",
       " 'butter',\n",
       " 'buy',\n",
       " 'by',\n",
       " 'cakes',\n",
       " 'calf',\n",
       " 'call',\n",
       " 'called',\n",
       " 'came',\n",
       " 'camel',\n",
       " 'camels',\n",
       " 'camest',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'canst',\n",
       " 'captain',\n",
       " 'captive',\n",
       " 'captives',\n",
       " 'carcases',\n",
       " 'carried',\n",
       " 'carry',\n",
       " 'cast',\n",
       " 'castles',\n",
       " 'catt',\n",
       " 'cattle',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'caused',\n",
       " 'cave',\n",
       " 'cease',\n",
       " 'ceased',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'chain',\n",
       " 'chamber',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'charge',\n",
       " 'charged',\n",
       " 'chariot',\n",
       " 'chariots',\n",
       " 'chesnut',\n",
       " 'chi',\n",
       " 'chief',\n",
       " 'child',\n",
       " 'childless',\n",
       " 'childr',\n",
       " 'children',\n",
       " 'chode',\n",
       " 'choice',\n",
       " 'chose',\n",
       " 'circumcis',\n",
       " 'circumcise',\n",
       " 'circumcised',\n",
       " 'citi',\n",
       " 'cities',\n",
       " 'city',\n",
       " 'clave',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'cleave',\n",
       " 'clo',\n",
       " 'closed',\n",
       " 'clothed',\n",
       " 'clothes',\n",
       " 'cloud',\n",
       " 'clusters',\n",
       " 'co',\n",
       " 'coat',\n",
       " 'coats',\n",
       " 'coffin',\n",
       " 'cold',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(text3))\n",
    "# “set” : removes the repetitions\n",
    "# “sorted” function sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f89a010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.050197203298673"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of the total number of tokens to the number of unique tokens\n",
    "len(text3) / len(set(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d4fd5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# occurrences of particular words\n",
    "text3.count(\"smote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c29a7a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01116968992940756"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of the number of occurrences of the word compared with the total number of words\n",
    "100 * text3.count('smote') / len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a8e492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Text -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aac4e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3b12ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'austen-emma.txt'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the first fileid (number 0 in the list) into a variable\n",
    "file1 = nltk.corpus.gutenberg.fileids( ) [0]\n",
    "file1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "769cf542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "887071"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the original text\n",
    "emmatext = nltk.corpus.gutenberg.raw(file1)\n",
    "len(emmatext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "897cea35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nan'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emmatext[:120] # the first 120 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3437fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# break the raw text into tokens\n",
    "# wordpunct_tokenize separates by white space and by special characters (punctuation)\n",
    "\n",
    "emmatokens = nltk.wordpunct_tokenize(emmatext)\n",
    "len(emmatokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25a94bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emmatokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a29cda72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to lower case\n",
    "emmawords = [w.lower( ) for w in emmatokens]\n",
    "emmawords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86c4af22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of emma words = 192427\n",
      "Length of emmavocab = 7344\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of emma words = \" + str(len(emmawords)))\n",
    "\n",
    "# getting the unique words and sorting them\n",
    "emmavocab = sorted(set(emmawords))\n",
    "print(\"Length of emmavocab = \" + str(len(emmavocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19dc6270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing text corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca8bb1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Categories: 15\n"
     ]
    }
   ],
   "source": [
    "# loads the Brown Corpus into the system memory\n",
    "from nltk.corpus import brown\n",
    "print ('Total Categories:', str(len(brown.categories())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6993458f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "# shows the various available categories\n",
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f835ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['There', 'were', 'thirty-eight', 'patients', 'on', 'the', 'bus', 'the', 'morning', 'I', 'left', 'for', 'Hanover', ',', 'most', 'of', 'them', 'disturbed', 'and', 'hallucinating', '.'], ['An', 'interne', ',', 'a', 'nurse', 'and', 'two', 'attendants', 'were', 'in', 'charge', 'of', 'us', '.'], ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the content of the category -> 'mystery'\n",
    "content = brown.sents(categories='mystery')\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7bf3e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('There', 'EX'), ('were', 'BED'), ('thirty-eight', 'CD'), ('patients', 'NNS'), ('on', 'IN'), ('the', 'AT'), ('bus', 'NN'), ('the', 'AT'), ('morning', 'NN'), ('I', 'PPSS'), ('left', 'VBD'), ('for', 'IN'), ('Hanover', 'NP'), (',', ','), ('most', 'AP'), ('of', 'IN'), ('them', 'PPO'), ('disturbed', 'VBN'), ('and', 'CC'), ('hallucinating', 'VBG'), ('.', '.')], [('An', 'AT'), ('interne', 'NN'), (',', ','), ('a', 'AT'), ('nurse', 'NN'), ('and', 'CC'), ('two', 'CD'), ('attendants', 'NNS'), ('were', 'BED'), ('in', 'IN'), ('charge', 'NN'), ('of', 'IN'), ('us', 'PPO'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the POS tags\n",
    "tagged_sentences = brown.tagged_sents(categories='mystery')\n",
    "tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c76fcece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There were thirty-eight patients on the bus the morning I left for Hanover , most of them disturbed and hallucinating .', 'An interne , a nurse and two attendants were in charge of us .', \"I felt lonely and depressed as I stared out the bus window at Chicago's grim , dirty West Side .\", 'It seemed incredible , as I listened to the monotonous drone of voices and smelled the fetid odors coming from the patients , that technically I was a ward of the state of Illinois , going to a hospital for the mentally ill .', 'I suddenly thought of Mary Jane Brennan , the way her pretty eyes could flash with anger , her quiet competence , the gentleness and sweetness that lay just beneath the surface of her defenses .']\n"
     ]
    }
   ],
   "source": [
    "# get sentences in natural form\n",
    "sentences = [' '.join(sentence_token) for sentence_token in content]\n",
    "print(sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "836c0d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 tagged words: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('There', 'EX'),\n",
       " ('were', 'BED'),\n",
       " ('thirty-eight', 'CD'),\n",
       " ('patients', 'NNS'),\n",
       " ('on', 'IN')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the top nouns in the mystery genre\n",
    "# nouns have either NN or NP in POS tag to indicate the various forms.\n",
    "tagged_words = brown.tagged_words(categories='mystery')\n",
    "print('first 5 tagged words: ')\n",
    "tagged_words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cf1442f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 nouns: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('patients', 'NNS'),\n",
       " ('bus', 'NN'),\n",
       " ('morning', 'NN'),\n",
       " ('Hanover', 'NP'),\n",
       " ('interne', 'NN')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns = [(word, tag) for word, tag in tagged_words if any(noun_tag in tag for noun_tag in ['NP', 'NN'])]\n",
    "print('first 5 nouns: ')\n",
    "nouns[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b483489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test/16118', 'test/18534', 'test/18540', 'test/18664', 'test/18665', 'test/18672', 'test/18911', 'test/19875', 'test/20106', 'test/20116', 'training/1035', 'training/1036', 'training/10602', 'training/10604', 'training/11170', 'training/11665', 'training/2618', 'training/29', 'training/3105', 'training/3708', 'training/3720', 'training/3723', 'training/3898', 'training/5883', 'training/5886', 'training/6000', 'training/6067', 'training/6197', 'training/7005', 'training/7006', 'training/7015', 'training/7036', 'training/7098', 'training/7099', 'training/9615']\n"
     ]
    }
   ],
   "source": [
    "# The Reuters Corpus categories are grouped into train and test sets.\n",
    "from nltk.corpus import reuters\n",
    "print(reuters.fileids(categories=['housing', 'income']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9667f0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['YUGOSLAV', 'ECONOMY', 'WORSENED', 'IN', '1986', ',', 'BANK', 'DATA', 'SHOWS', 'National', 'Bank', 'economic', 'data', 'for', '1986', 'shows', 'that', 'Yugoslavia', \"'\", 's', 'trade', 'deficit', 'grew', ',', 'the', 'inflation', 'rate', 'rose', ',', 'wages', 'were', 'sharply', 'higher', ',', 'the', 'money', 'supply', 'expanded', 'and', 'the', 'value', 'of', 'the', 'dinar', 'fell', '.'], ['The', 'trade', 'deficit', 'for', '1986', 'was', '2', '.', '012', 'billion', 'dlrs', ',', '25', '.', '7', 'pct', 'higher', 'than', 'in', '1985', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "print(reuters.sents(fileids=[u'test/16118',\tu'test/18534']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f0382df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('hike.n.01'), Synset('rise.n.09'), Synset('raise.n.01'), Synset('hike.v.01'), Synset('hike.v.02')]\n"
     ]
    }
   ],
   "source": [
    "# WordNet corpus has words that are semantically linked synsets\n",
    "from nltk.corpus import wordnet as wn\n",
    "word = 'hike' # taking hike as our word of interest\n",
    "\n",
    "# get word synsets\n",
    "word_synsets = wn.synsets(word)\n",
    "print(word_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2435a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset Name: hike.n.01\n",
      "POS Tag: n\n",
      "Definition: a long walk usually for exercise or pleasure\n",
      "Examples: ['she enjoys a hike in her spare time']\n",
      "\n",
      "Synset Name: rise.n.09\n",
      "POS Tag: n\n",
      "Definition: an increase in cost\n",
      "Examples: ['they asked for a 10% rise in rates']\n",
      "\n",
      "Synset Name: raise.n.01\n",
      "POS Tag: n\n",
      "Definition: the amount a salary is increased\n",
      "Examples: ['he got a 3% raise', 'he got a wage hike']\n",
      "\n",
      "Synset Name: hike.v.01\n",
      "POS Tag: v\n",
      "Definition: increase\n",
      "Examples: ['The landlord hiked up the rents']\n",
      "\n",
      "Synset Name: hike.v.02\n",
      "POS Tag: v\n",
      "Definition: walk a long way, as for pleasure or physical exercise\n",
      "Examples: ['We were hiking in Colorado', 'hike the Rockies']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get details for each synonym in synset\n",
    "for synset in word_synsets:\n",
    "    print('Synset Name:', synset.name())\n",
    "    print('POS Tag:', synset.pos())\n",
    "    print('Definition:', synset.definition())\n",
    "    print('Examples:', synset.examples())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a0a636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distributions (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0c4469a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 11454),\n",
       " ('.', 6928),\n",
       " ('to', 5239),\n",
       " ('the', 5201),\n",
       " ('and', 4896),\n",
       " ('of', 4291),\n",
       " ('i', 3178),\n",
       " ('a', 3129),\n",
       " ('it', 2528),\n",
       " ('her', 2469)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the set of keys are all the words, and the set of values are the frequency (count) of each word\n",
    "\n",
    "fdist = FreqDist(emmawords)\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30cbe3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "865"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the frequencies of individual words\n",
    "fdist['emma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f02df1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# “makeAlphaFreqDist” \n",
    "# takes a list of words as an argument\n",
    "# returns a Frequency Distribution which only contains words with all alphabetical characters.\n",
    "import re\n",
    "\n",
    "def makeAlphaFreqDist(words):\n",
    "    adist = FreqDist() # Make a new empty frequency distribution called adist\n",
    "    pattern = re.compile('.*[^a---z].*') # match any word that contains a non-alphabetical character\n",
    "    \n",
    "    for word in words:\n",
    "        if not pattern.match(word): # doesn’t contain any non-alphabetical characters\n",
    "            adist.update([word]) # add it to the frequency distribution\n",
    "    return adis\n",
    "\n",
    "# .update -: adds the word and adds one to its count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6371db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emmawords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "824320ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adist = makeAlphaFreqDist(emmawords)\n",
    "# common_words = adist.most_common(10)\n",
    "# print(str(len(common_words)))\n",
    "\n",
    "# for word, freq in adist.most_common(10):\n",
    "#     print(word, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5929a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting words and n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8de97f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "file0 = nltk.corpus.gutenberg.fileids( ) [0]\n",
    "emmatext = nltk.corpus.gutenberg.raw(file0) # get the text of the book Emma\n",
    "emmatokens = nltk.wordpunct_tokenize(emmatext) # separates emmatext into tokens\n",
    "emmawords = [w.lower( ) for w in emmatokens] # converts all the characters to lower case\n",
    "shortwords = emmawords[11:111] # a list with only the first 101 words\n",
    "shortwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09f585f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortdist = FreqDist(shortwords) # create a frequency distribution of the words\n",
    "words_in_decreasing_freq = shortdist.keys( ) # produces the list of words in order of decreasing frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97a2bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma 1\n",
      "woodhouse 1\n",
      ", 8\n",
      "handsome 1\n",
      "clever 1\n",
      "and 4\n",
      "rich 1\n",
      "with 2\n",
      "a 3\n",
      "comfortable 1\n"
     ]
    }
   ],
   "source": [
    "for word in list(words_in_decreasing_freq)[:10]:\n",
    "    print (word, shortdist[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "943770cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More Specialized Frequency Distributions (What is a word?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c35bca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonAlphaMatch to -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='-'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordPunct tokenization produces tokens that have special characters\n",
    "# Following code remove all the tokens that have special characters and leave only tokens that consist of all alphabetic characters\n",
    "\n",
    "pattern = re.compile('.*[^a-z].*') # matches any token that contains a non-alphabetical character\n",
    "nonAlphaMatch = pattern.match('-')\n",
    "\n",
    "print('nonAlphaMatch to -')\n",
    "nonAlphaMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f603c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ : matched non-alphabetical\n",
      "Moby : matched non-alphabetical\n",
      "Dick : matched non-alphabetical\n",
      "by : matched non-alphabetical\n",
      "Herman : matched non-alphabetical\n",
      "Melville : matched non-alphabetical\n",
      "1851 : matched non-alphabetical\n",
      "] : matched non-alphabetical\n",
      "ETYMOLOGY : matched non-alphabetical\n",
      ". : matched non-alphabetical\n"
     ]
    }
   ],
   "source": [
    "for word in text1[:10]:\n",
    "    if nonAlphaMatch: \n",
    "        print (word, ': matched non-alphabetical')\n",
    "    else:\n",
    "        print (word, ': NOT matched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "552e0217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words-: list of the common words that appear with great frequenc\n",
    "\n",
    "stopwords = ['to', 'be', 'of', 'the', 'in', 'it', 'was',\n",
    "'i', 'am', 'she', 'had', 'been', 'is', 'have','could', 'not',\n",
    "'her', 'he', 'do', 'and', 'would', 'such', 'a', 'his', 'must']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f77d78b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a frequency distribution from a list of tokens that has no tokens \n",
    "# containing non-alphabetical characters or words in the stopword list.\n",
    "\n",
    "def alphaStopFreqDist(words, stoplist):\n",
    "    asdist = FreqDist()\n",
    "    pattern = re.compile('.*[^a-z].*')\n",
    "    for word in words:\n",
    "        if not pattern.match(word):\n",
    "            if not word in stoplist:\n",
    "                asdist[word.lower()] += 1\n",
    "    return asdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1853a0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'woodhouse',\n",
       " 'handsome',\n",
       " 'clever',\n",
       " 'rich',\n",
       " 'with',\n",
       " 'comfortable',\n",
       " 'home',\n",
       " 'happy',\n",
       " 'disposition']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdist = alphaStopFreqDist(shortwords, stopwords)\n",
    "list(asdist.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5afca367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma 1\n",
      "woodhouse 1\n",
      "handsome 1\n",
      "clever 1\n",
      "rich 1\n",
      "with 2\n",
      "comfortable 1\n",
      "home 1\n",
      "happy 1\n",
      "disposition 1\n"
     ]
    }
   ],
   "source": [
    "for key in list(asdist.keys())[:10]:\n",
    "        print (key, asdist[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c02e0795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Frequency Distributions\n",
    "\n",
    "# look at pairs of words that are frequently collocated\n",
    "# WORDS THAT occur in a sequence called a 'bigram'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f4d70f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigramDist(words, stoplist):\n",
    "    biDist = FreqDist()\n",
    "    uniDist = alphaStopFreqDist(words, stoplist) # restrict words to those that occur in a unigram/word frequency distribution\n",
    "# without non-alphabetical characters and stop words.\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i-1] in uniDist and words[i] in uniDist:\n",
    "            biword = words[i-1] + ' ' + words[i]\n",
    "            biDist[biword.lower()] += 1\n",
    "    return biDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "275431ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['emma woodhouse', 'comfortable home', 'happy disposition', 'unite some', 'best blessings', 'lived nearly', 'nearly twenty', 'one years', 'world with', 'with very', 'very little', 'distress or', 'or vex', 'two daughters', 'most affectionate', 'indulgent father', 's marriage', 'house from', 'very early', 'early period', 'died too', 'too long', 'long ago', 'ago for'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out bigram function on shortwords and emmawords\n",
    "\n",
    "shortbidist = bigramDist(shortwords, stopwords)\n",
    "shortbidist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c55f3dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma by 1\n",
      "by jane 2\n",
      "jane austen 1\n",
      "emma woodhouse 5\n",
      "comfortable home 2\n",
      "happy disposition 1\n",
      "unite some 1\n",
      "best blessings 2\n",
      "lived nearly 1\n",
      "nearly twenty 1\n"
     ]
    }
   ],
   "source": [
    "emmabidist = bigramDist(emmawords, stopwords)\n",
    "for key in list(emmabidist.keys())[:10]:\n",
    "    print (key, emmabidist[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "761c8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "00d04f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "ancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bcdcac65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volum', 'i', 'chapter']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emmaregstem = [porter.stem(t) for t in emmatokens]\n",
    "emmaregstem[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4cfea6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volum', 'i', 'chapter']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emmalowerstem = [porter.stem(t) for t in emmawords]\n",
    "emmalowerstem[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "116cf2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'emm', 'by', 'jan', 'aust', '1816', ']', 'volum', 'i', 'chapt']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emmaregstem = [ancaster.stem(t) for t in emmatokens]\n",
    "emmaregstem[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "27c46059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'emm', 'by', 'jan', 'aust', '1816', ']', 'volum', 'i', 'chapt']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emmalowerstem = [ancaster.stem(t) for t in emmawords]\n",
    "emmalowerstem[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "68150f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'friend'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our own stemmer by making a list of suffixes to take off.\n",
    "\n",
    "def stem(word):\n",
    "    for suffix in ['ing','ly','ed','ious','ies','ive','es','s']:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "stemmedword=stem('friends')\n",
    "stemmedword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "96dfca7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "emmalemma=[wnl.lemmatize(t) for t in emmawords]\n",
    "emmalemma[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "295b80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expressions and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fdd9f80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import *\n",
    "file0 = nltk.corpus.gutenberg.fileids( ) [0]\n",
    "emmatext = nltk.corpus.gutenberg.raw(file0)\n",
    "\n",
    "type(emmatext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "19589a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "E\n",
      "m\n",
      "m\n",
      "a\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "J\n"
     ]
    }
   ],
   "source": [
    "shorttext = emmatext[:150]\n",
    "for char in shorttext[:10]:\n",
    "    print (char)\n",
    "# Strings can be treated as lists of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b2af6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python and the Holy Grail'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate strings together\n",
    "string1 = 'Monty Python'\n",
    "string2 = 'Holy Grail'\n",
    "string1 + string2\n",
    "string1 + ' and the ' + string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "16f47dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Emma by Jane Austen 1816]  VOLUME I  CHAPTER I   Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace all the new characters ‘\\n’ with a space ' '.\n",
    "newemmatext = emmatext.replace('\\n', ' ')\n",
    "shorttext = newemmatext[:150]\n",
    "shorttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "18b122ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expressions for Tokenizing Text\n",
    "# re.match -: finds any match at the beginning of a string\n",
    "# re.search -: finds a match anywhere in the string, and\n",
    "# re.findall -: find the substrings that matched anywhere in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8afd6eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pword = re.compile('\\w+') # find on the alphabetic words of this simple text\n",
    "re_pwd = re.findall(pword, shorttext)\n",
    "len(re_pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "360239a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['U', 'S', 'A', 'poster', 'print', 'costs', '12', '40', 'with', '10', 'off']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specialtext = 'U.S.A. poster-print costs $12.40, with 10% off.'\n",
    "re.findall(pword, specialtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dbe9a683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('U', ''),\n",
       " ('S', ''),\n",
       " ('A', ''),\n",
       " ('poster-print', '-print'),\n",
       " ('costs', ''),\n",
       " ('12', ''),\n",
       " ('40', ''),\n",
       " ('with', ''),\n",
       " ('10', ''),\n",
       " ('off', '')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matching words can have an internal hyphen\n",
    "ptoken = re.compile('(\\w+(-\\w+)*)')\n",
    "# put parentheses around the part of the pattern that can be repeated >=0\n",
    "# But, then findall will only report the part that matched inside those parentheses,\n",
    "# So put an extra pair of parentheses around the whole match.\n",
    "re.findall(ptoken, specialtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "99878d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.findall reports both the whole matched text and the internal matched text. \n",
    "# can fix it by using the re.groups function to access only the outer match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28222e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('U.S.A.', 'A.')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match abbreviations that might have a “.” inside\n",
    "pabbrev = re.compile('(([A-Z]\\.)+)')\n",
    "re.findall(pabbrev, specialtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0ab7cd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('poster-print ', '-print', ''), ('costs ', '', ''), ('with ', '', '')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match either words or abbreviations\n",
    "ptoken = re.compile('(\\w+(-\\w+)* | ([A-Z]\\.)+)')\n",
    "re.findall(ptoken, specialtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "271820d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('U.S.A.', 'A.', ''),\n",
       " ('poster-print', '', '-print'),\n",
       " ('costs', '', ''),\n",
       " ('12', '', ''),\n",
       " ('40', '', ''),\n",
       " ('with', '', ''),\n",
       " ('10', '', ''),\n",
       " ('off', '', '')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptoken = re.compile('(([A-Z]\\.)+|\\w+(-\\w+)*)')\n",
    "re.findall(ptoken, specialtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9966ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That didn’t work because it first found the alphabetic words which found ‘U’, ‘S’ and ‘A’ as separate words before it could match the abbreviations.\n",
    "# Therefore, the order of the matching patterns is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "82447069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('U.S.A.', 'A.', '', ''), ('poster-print', '', '-print', ''), ('costs', '', '', ''), ('$12.40', '', '', '.40'), ('with', '', '', ''), ('10', '', '', ''), ('off', '', '', '')]\n"
     ]
    }
   ],
   "source": [
    "ptoken = re.compile(r'(([A-Z]\\.)+|\\w+(-\\w+)*|\\$?\\d+(\\.\\d+)?)')\n",
    "output = re.findall(ptoken, specialtext)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e7981d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‘r’ in front of the pattern -: Python’s notation for a raw string. This accepts ‘\\’ as itself in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6fe1e681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A.', '', ''),\n",
       " ('', '-print', ''),\n",
       " ('', '', ''),\n",
       " ('', '', '.40'),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', '')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an expression to match the currency\n",
    "ptoken = re.compile(r'''([A-Z]\\.)+\n",
    "    | \\w+(-\\w+)* \n",
    "    | \\$?\\d+(\\.\\d+)? \n",
    "    ''', re.X)\n",
    "\n",
    "re.findall(ptoken, specialtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9f2c28ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expression Tokenizer using NLTK Tokenizer\n",
    "\n",
    "# Regular expressions can also be written down in the “verbose” version, using the (?x) flag \n",
    "# allows the alternatives to be on different lines with comments\n",
    "# alleviates the need to put extra parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "620e10d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' (?x) # set flag to allow verbose regexps\\n    ([A-Z]\\\\.)+ # abbreviations, e.g. U.S.A\\n    | \\\\w+(-\\\\w+)*  # words with internal hyphens\\n    | \\\\$?\\\\d+(\\\\.\\\\d+)?%?  # currency and percentages, $12.40, 50%\\n    | \\\\.\\\\.\\\\. # ellipsis\\n    | [][.,;\"\\'?():-_\\'] # separate special character tokens\\n    '"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r''' (?x) # set flag to allow verbose regexps\n",
    "    ([A-Z]\\.)+ # abbreviations, e.g. U.S.A\n",
    "    | \\w+(-\\w+)*  # words with internal hyphens\n",
    "    | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, $12.40, 50%\n",
    "    | \\.\\.\\. # ellipsis\n",
    "    | [][.,;\"'?():-_'] # separate special character tokens\n",
    "    '''\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ddefd948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the expression to separate special characters as individual tokens comes last in the list\n",
    "# because other expressions, such as the words with internal hyphens, can first get longer tokens that involve individual characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "81d83cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', '')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize(shorttext, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "83759966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A.', '', ''),\n",
       " ('', '-print', ''),\n",
       " ('', '', ''),\n",
       " ('', '', '.40'),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', ''),\n",
       " ('', '', '')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize(specialtext, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c4789481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In baove two code segments if there are any characters not matched by one of the regular expression patterns, then it is omitted as a token in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "18abb7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetPattern = r''' (?x) # set flag to allow verbose regexps (https?://|www)\\S+ # simple URLs\n",
    "    | (:-\\)|;-\\)) # small list of emoticons\n",
    "    | &(amp|lt|gt|quot); # XML or HTML entity | \\#\\w+ # hashtags | @\\w+ # mentions\n",
    "    | \\d+:\\d+ # timelike pattern | \\d+\\.\\d+ # number with a decimal\n",
    "    | (\\d+,)+?\\d{3}(?=([^,]|$)) # number with a comma\n",
    "    | ([A-Z]\\.)+ # simple abbreviations\n",
    "    | (--+) # multiple dashes\n",
    "    | \\w+(-\\w+)* # words with internal hyphens or apostrophes | ['\\\".?!,:;]+ # special characters\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "70eb5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1 = \"@natalieohayre I agree #hc09 needs reform- but not by crooked politicians who r clueless about healthcare! #tcot #fishy NO GOV'T TAKEOVER!\"\n",
    "\n",
    "tweet2 = \"To Sen. Roland Burris: Affordable, quality health insurance can't wait http://bit.ly/j63je #hc09 #IL #60660\"\n",
    "\n",
    "tweet3 = \"RT @karoli: RT @Seriou: .@whitehouse I will stand w/ Obama on #healthcare, I trust him. #p2 #tlot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "968495e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', '')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize(tweet1, tweetPattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b6c5ef60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', '')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize(tweet2, tweetPattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5f785535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', ''),\n",
       " ('', '', '', '', '', '', '')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize(tweet3, tweetPattern)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
